# Inference

We demonstrate how to run inference (next token prediction) with the GPT base model in the [`generate.py`](generate.py) script:

```bash
python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

Output:

```text
Hello, my name is Levi Durrer, I'm an Austrian journalist - Chairman of the Press Blair Party, with 37 years in the Press Blair International, and two years in the Spectre of Austerity for the other. I'm crossing my fingers that you will feel
```

The script assumes you have downloaded and converted the weights as described [here](download_stablelm.md).

This will run the 3B pre-trained model and require ~7 GB of GPU memory using the `bfloat16` datatype.

## Run interactively

You can also chat with the model interactively:

```bash
python chat/base.py --checkpoint_dir checkpoints/stabilityai/stablelm-tuned-alpha-3b
```

This script can work with any checkpoint. For the best chat-like experience, we recommend using it with a checkpoints
fine-tuned for chatting such as `stabilityai/stablelm-tuned-alpha-3b` or `togethercomputer/RedPajama-INCITE-Chat-3B-v1`.

## Run a large model on one smaller device

Check out our [quantization tutorial](quantize.md).

## Run a large model on multiple smaller devices

You can also use the Fully-Sharded Data Parallel (FSDP) distributed strategy to leverage multiple devices to perform inference. This will allow you to run models that wouldn't fit in a single card by sharding them across several.

For instance, `falcon-40b` would require ~80 GB of GPU memory to run on a single device. We can instead run it on 4 A100 40GB GPUs:

```shell
python generate/base.py --checkpoint_dir checkpoints/tiiuae/falcon-40b --strategy fsdp --devices 4
```

Which will take 32 GB of memory, and run at 0.37 tokens/sec.

Or to reduce the memory requirements even further, you can try using CPU offloading. For that, you will need to manually edit the `cpu_offload=False` parameter in the file and set it to `True`.

Now we can run it on just 2 devices.

```shell
python generate/base.py --checkpoint_dir checkpoints/tiiuae/falcon-40b --strategy fsdp --devices 2
```

taking 13 GB of memory but running at 0.12 tokens/sec on 2 A100 40GB GPUs.
Smaller devices like 3090s (24 GB) can also fit it with this technique.
## Download [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) weights

Vicuna is an open-source family of chatbots trained by fine-tuning LLaMA on user-shared conversations collected from [ShareGPT](https://sharegpt.com).

To see all the available checkpoints for Vicuna, run:

```bash
python scripts/download.py | grep vicuna
```

which will print

```text
lmsys/vicuna-7b-v1.3
lmsys/vicuna-13b-v1.3
lmsys/vicuna-33b-v1.3
lmsys/vicuna-7b-v1.5
lmsys/vicuna-7b-v1.5-16k
lmsys/vicuna-13b-v1.5
lmsys/vicuna-13b-v1.5-16k
```

In order to use a specific Vicuna checkpoint, for instance [vicuna-7b-v1.5](https://huggingface.co/lmsys/vicuna-7b-v1.5), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id lmsys/vicuna-7b-v1.5

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/lmsys/vicuna-7b-v1.5
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/lmsys/vicuna-7b-v1.5
```
# NeurIPS 2023 LLM Efficiency Challenge Quickstart Guide



The [NeurIPS 2023 Efficiency Challenge](https://llm-efficiency-challenge.github.io/) is a competition focused on training **1 LLM for 24 hours on 1 GPU** – the team with the best LLM gets to present their results at NeurIPS 2023.

This quick start guide is a short starter guide illustrating the main steps to get started with Lit-GPT, which was selected as the competition's official starter kit.



&nbsp;

## Competition Facts


&nbsp;

**Permitted GPUs:**

- 1x A100 (40 GB RAM);
- 1x RTX 4090 (24 GB RAM).

&nbsp;

**Permitted models:**

- All transformer-based LLM base models that are not finetuned yet.

The subset of Lit-GPT models supported in this competition is listed in the table below.
These don't include models that have been finetuned or otherwise aligned, as per the rules of the challenge.

&nbsp;

| Models in Lit-GPT         | Reference                                                    |
| ------------------------- | ------------------------------------------------------------ |
| Meta AI Llama 2 Base      | [Touvron et al. 2023](https://arxiv.org/abs/2307.09288)      |
| TII UAE Falcon Base       | [TII 2023](https://falconllm.tii.ae/)                        |
| OpenLM Research OpenLLaMA | [Geng & Liu 2023](https://github.com/openlm-research/open_llama) |
| EleutherAI Pythia         | [Biderman et al. 2023](https://arxiv.org/abs/2304.01373)     |
| StabilityAI StableLM Base | [Stability AI 2023](https://github.com/Stability-AI/StableLM) |

&nbsp;

**Permitted datasets**

Any open-source dataset is allowed. Originally, [per competition rules](https://llm-efficiency-challenge.github.io/challenge), datasets that utilize "generated content" from other LLMs were not permitted. However, the rules were recently softened to also allow LLM-generated datasets if those datasets are made available and if it is not against the usage restrictions and guidelines of the LLM. If you plan to use a specific dataset that is not explicitely listed on the [challenge website](https://llm-efficiency-challenge.github.io/challenge) or want to use LLM-generated data, it is recommended to reach out to the organizers and confirm that this is in line with the competition rules.

Examples of permitted datasets are the following:

- [Databricks-Dolly-15](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- [OpenAssistant Conversations Dataset (oasst1)](https://huggingface.co/datasets/OpenAssistant/oasst1)
- [The Flan Collection](https://github.com/google-research/FLAN/tree/main/flan/v2)

You are allowed to create your own datasets if they are made
publicly accessible under an open-source license, and they are not generated from other LLMs (even open-source ones).

Helpful competition rules relevant to the dataset choice:

- The maximum prompt/completion length the models are expected to handle is 2048 tokens.
- The evaluation will be on English texts only.

&nbsp;

**Submission deadline**

- October 25, 2023 ([Please check](https://llm-efficiency-challenge.github.io/dates) official website in case of updates.)

&nbsp;

## Lit-GPT Setup

Use the following steps to set up the Lit-GPT repository on your machine.

```shell
git clone https://github.com/Lightning-AI/lit-gpt
cd lit-gpt
pip install -r requirements.txt tokenizers sentencepiece huggingface_hub
```

&nbsp;

## Downloading Model Checkpoints

This section explains how to download the StableLM 3B Base model, one of the smallest models supported in Lit-GPT (an even smaller, supported model is Pythia, which starts at 70M parameters). The downloaded and converted checkpoints will occupy approximately 28 Gb of disk space.

```bash
python scripts/download.py \
  --repo_id stabilityai/stablelm-base-alpha-3b

python scripts/convert_hf_checkpoint.py \
  --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

While StableLM 3B Base is useful as a first starter model to set things up, you may want to use the more capable Falcon 7B or Llama 2 7B/13B models later. See the [`download_*`](https://github.com/Lightning-AI/lit-gpt/tree/main/tutorials) tutorials in Lit-GPT to download other model checkpoints.

After downloading and converting the model checkpoint, you can test the model via the following command:

```bash
python generate/base.py \
  --prompt "LLM efficiency competitions are fun, because" \
  --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

&nbsp;

## Downloading and Preparing Datasets

The following command will download and preprocess the Dolly15k dataset for the StableLM 3B Base model:

```bash
python scripts/prepare_dolly.py \
  --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b \
  --destination_path data/dolly-stablelm3b
```

> [!NOTE]
> The preprocessed dataset is specific to the StableLM 3B model. If you use a different model like Falcon or Llama 2 later, you'll need to process the dataset with that model checkpoint directory. This is because each model uses a different tokenizer.

&nbsp;

## Finetuning

[Low-rank Adaptation (LoRA)](https://lightning.ai/pages/community/tutorial/lora-llm/) is a good choice for a first finetuning run. The Dolly dataset has ~15k samples, and the finetuning might take half an hour.

To accelerate this for testing purposes, edit the [./finetune/lora.py](https://github.com/Lightning-AI/lit-gpt/blob/main/finetune/lora.py) script and change `max_iters = 50000` to `max_iters = 500` at the top of the file.

> [!NOTE]
> The Dolly dataset has a relatively long context length, which could result in out-of-memory issues. The maximum context length that is used for the evaluation, [according to the official competition rules](https://llm-efficiency-challenge.github.io/question), is 2,048 tokens. Hence, it's highly recommended to prepare the dataset with a fixed max length, for example, `python scripts/prepare_dolly.py --max_seq_length 2048`.

The following command finetunes the model:

```bash
CUDA_VISIBLE_DEVICES=2 python finetune/lora.py \
  --data_dir data/dolly-stablelm3b \
  --checkpoint_dir "checkpoints/stabilityai/stablelm-base-alpha-3b" \
  --out_dir "out/stablelm3b/dolly/lora/experiment1" \
  --precision "bf16-true"
```

With 500 iterations, this takes approximately 1-2 min on an A100 and uses 26.30 GB GPU memory.

If you are using an RTX 4090, change `micro_batch_size=4` to `micro_batch_size=1` so that the model will only use 12.01 GB of memory.

(More finetuning settings are explained [here](https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide/#toc10).)

&nbsp;

## Local Evaluation

The official Lit-GPT competition will use a small subset of HELM tasks for model evaluation, which includes BigBench (general), MMLU (knowledge), TruthfulQA (knowledge and harm in a multiple choice format), CNN/DailyMail (news summarization), GSM8K (math), and BBQ (bias).

HELM is currently also being integrated into Lit-GPT to evaluate LLMs before submission.

However, a tool with a more convenient interface is Eleuther AI's [Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness), which contains some tasks, for example, BigBench, TruthfulQA, and GSM8k, that overlap with HELM. We can set up the Evaluation Harness as follows:

```bash
pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git@master
```

And then we can use it via the following command:

```bash
python eval/lm_eval_harness.py \
  --checkpoint_dir "checkpoints/stabilityai/stablelm-base-alpha-3b" \
  --precision "bf16-true" \
  --eval_tasks "[truthfulqa_mc,gsm8k]" \
  --batch_size 4 \
  --save_filepath "results-stablelm-3b.json"
```

(You can find a full task list in the task table [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).)

To evaluate a LoRA-finetuned model, you need to first merge the LoRA weights with the base model to create a new checkpoint file:


```bash
python scripts/merge_lora.py \
  --checkpoint_dir "checkpoints/stabilityai/stablelm-base-alpha-3b/" \
  --lora_path "out/stablelm3b/dolly/lora/experiment1/lit_model_lora_finetuned.pth" \
  --out_dir "out/lora_merged/stablelm-base-alpha-3b/"
```

```bash
cp checkpoints/stabilityai/stablelm-base-alpha-3b/*.json \
out/lora_merged/stablelm-base-alpha-3b/
```

For more information on LoRA weight merging, please see the
[Merging LoRA Weights](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_lora.md#merging-lora-weights)
section of the LoRA finetuning documentation.

After merging the weights, we can use the `lm_eval_harness.py` similar to before with the only difference that we now use the new
checkpoint folder containing the merged LoRA model:

```bash
python eval/lm_eval_harness.py \
  --checkpoint_dir "out/lora_merged/stablelm-base-alpha-3b" \
  --precision "bf16-true" \
  --eval_tasks "[truthfulqa_mc,gsm8k]" \
  --batch_size 4 \
  --save_filepath "results-stablelm-3b.json"
```

&nbsp;

## Submission

You will be required to submit a Docker image for the submission itself. Fortunately, the organizers have a GitHub repository with the exact steps [here](https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge) and a toy-submission setup guide to test your model locally before submission.


&nbsp;

## Additional Information & Resources

- [The official NeurIPS 2023 LLM Efficiency Challenge competition website](https://llm-efficiency-challenge.github.io/)
- A more extensive guide, including environment setup tips: [The NeurIPS 2023 LLM Efficiency Challenge Starter Guide](https://lightning.ai/pages/community/tutorial/neurips2023-llm-efficiency-guide)
- [Official competition Discord](https://discord.com/login?redirect_to=%2Fchannels%2F1077906959069626439%2F1134560480795570186) and [Lightning AI + Lit-GPT Discord](https://discord.com/invite/MWAEvnC5fU)
- LoRA vs Adapter vs Adapter v2 comparison in Lit-GPT using Falcon 7B: [Finetuning Falcon LLMs More Efficiently With LoRA and Adapters](https://lightning.ai/pages/community/finetuning-falcon-efficiently/)
- [Dealing with out-of-memory (OOM) errors in Lit-GPT](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/oom.md)
- Introduction to Fabric (an API to access more advanced PyTorch features used in Lit-GPT) and memory saving tips: [Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorch](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/)
## Dealing with out-of-memory (OOM) errors

If you got this error while running a script

```bash
OutOfMemoryError: CUDA out of memory. Tried to allocate 2.22 GiB. GPU 0 has a total capacity of 79.15 GiB of which 228.38 MiB is free. Including non-PyTorch memory, this process
has 78.93 GiB memory in use. Of the allocated memory 76.28 GiB is allocated by PyTorch, and 2.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory
is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

it means that your GPU memory size wasn't big enough for the model and script configuration.

Here's a few things you can try:

### Reduce the micro batch size

Adjust the `micro_batch_size = ...` variable in the fine-tuning and pretraining scripts. This variable determines the number of samples loaded per iteration.

A smaller value will simply load fewer samples simultaneously. The minimum value is 1.

Experiment with different micro batch sizes to find a balance between memory consumption and computational efficiency. Smaller micro batch sizes consume less memory but may result in slower training convergence. Conversely, larger micro batch sizes require more memory but can accelerate training speed.

### Reduce the model's context length

The context length (`block_size` in the code) plays a significant role in running models with attention.
* The pretraining scripts are configured to use the full context length of the model to train. 
* The finetuning scripts are configured to use the longest sample length of the training data to avoid allocating unnecessary memory (`max_seq_length` in the code).
  If that's longer than the model's context length, an error is raised. If you try to run a batch that is longer than this, an error is raised. 

However, your hardware may not support such large context lengths. Here's what you can do:
* For the pretraining scripts, you can simply reduce the `Config(block_size=...)` value.
* For the finetuning scripts, you can trim the length of the samples in your dataset. 
  Most of the `scripts/prepare_*.py` scripts expose a `--max_seq_length=...` argument. This might also be useful in cases where
  sample lengths are highly unbalanced, as the presence of a single very long sample would incur a larger memory usage for all other
  shorter samples. For example, the median length of the samples in Alpaca is 110 tokens. Truncating the Alpaca dataset to 256 max tokens reduces the memory requirements of a Falcon 7B model from 23.52 GB to 15.73 GB. For more information about the dataset truncation, please see the *Truncating datasets* section in the the [prepare_datasets.md](prepare_datasets.md) tutorial.

Keep in mind that reducing the context length will affect the modelling performance on text sequences longer than the limit.

### Use lower precision

Our scripts expose the `--precision` argument, this directly impacts the memory usage.

Using true lower precision (`16-true`, `bf16-true`) reduces the memory usage by half compared to `32-true`, however,
the model might start producing NaNs due to the limited range of representable values.

Mixed precision training (`16-mixed`, `bf16-mixed`) provides better stability but offers limited memory reduction.

### Do sharding across multiple GPUs

For exceptionally large models, the aforementioned techniques might still not suffice. If you have multiple GPUs available,
you can trade off memory for speed by changing the `devices = 1` argument in the scripts. Enabling this option enables a parallelism technique (FSDP), sharding the memory across different GPUs.

The default configuration already uses activation checkpointing, but you can enable CPU offloading by changing the `cpu_offload=False` argument in the scripts.

### Try a different optimizer

Our scripts use the [`AdamW` optimizer](https://pytorch.org/docs/main/generated/torch.optim.AdamW.html).
It maintains 2 states for each trainable parameter of the model, meaning that the optimizer memory is double compared to
an optimizer like [`SGD`](https://pytorch.org/docs/main/generated/torch.optim.SGD.html).

You can try replacing it with your optimizer of choice that is lighter in memory requirements. Keep in mind that different optimizers have distinct optimization behaviors, so it's essential to assess their impact on the training process and model performance.
An example would be the recently published [Sophia](https://arxiv.org/abs/2305.14342) or [Lion](https://arxiv.org/abs/2302.06675) optimizers.

This suggestion is particularly relevant for pretraining, as the trainable parameters in the model represent a small
subset of the total in the fine-tuning scripts.
# Pretrain Llama 2 on OpenWebText

This tutorial will walk you through setting up the OpenWebText dataset and launching the pretraining script.

## What's OpenWebText

[OpenWebText](https://github.com/jcpeterson/openwebtext) is an open-source reproduction of OpenAI's unreleased WebText training dataset, which was originally used to train GPT-2. The version that is used here consists of 8M documents and is loaded via the `load_dataset("openwebtext", ...)` function from the [datasets](https://github.com/huggingface/datasets) Python package. [Please refer to the website hosting the dataset](https://huggingface.co/datasets/Skylion007/openwebtext) for the licensing information.


## Prepare OpenWebText for training


In order to start pretraining lit-gpt on it, you need to read, tokenize, and write the data in binary format.

To prepare the dataset with the Llama 2 tokenizer, run

```bash
pip install datasets

python scripts/prepare_openwebtext.py \
  --checkpoint_dir checkpoints/meta-llama/Llama-2-7b-hf/ \
  --destination_path data/openwebtext
```

The script will take about 15 min to run.


## Pretraining

Running the pretraining script with its default settings requires at least 4 GPUs with 40GB+ each. (However, alternatively, you can train a smaller Pythia-70m on 1 GPU, more information about that further below).

```bash
python pretrain/openwebtext.py \
  --devices 4
```

The script will save checkpoints periodically to the folder `out/`.

By default, the `pretrain/openwebtext.py` script will pretrain the Llama 2 7B model with FSDP in
`bfloat16` precision and gradient accumulation.

You can easily change the size of the model by passing a different string to the model name variable

```python
model_name = "Llama-2-7b-hf"
```

at the top of this script.

The currently supported model names are contained in the [config.py](https://github.com/Lightning-AI/lit-gpt/lit_gpt/config.py) file. 
You can 

1) either search this file for lines containing "name =",
2) or run `python scripts/download.py` without additional command line arguments,

Keep in mind that the original LLaMA training for the 7B model required 83k A100 80GB
hours (on a bigger dataset). However, for full pretraining on OpenWebText, you'll likely still need access to a cluster.

Once you're in a cluster, you can follow [these instructions](https://lightning.ai/docs/fabric/stable/fundamentals/launch.html#launch-on-a-cluster)
to launch the script across machines:

- [SLURM cluster](https://lightning.ai/docs/fabric/stable/guide/multi_node/slurm.html)
- [Barebones cluster](https://lightning.ai/docs/fabric/stable/guide/multi_node/barebones.html)
- [MPI](https://lightning.ai/docs/fabric/stable/guide/multi_node/other.html)

The [script contains several configurations and hyperparameters](https://github.com/Lightning-AI/lit-gpt/blob/main/pretrain/redpajama.py#L23-L45) you can tweak.

For instance, `micro_batch_size` should be adjusted so the process will use the available
GPU memory. For more tips to avoid out-of-memory issues, please also see the more detailed
[Dealing with out-of-memory (OOM) errors](oom.md) guide.

Last, logging is kept minimal in the script. In order to use a particular logger
please refer to <https://lightning.ai/docs/fabric/stable/api/loggers.html> or
call a logging client library like `wandb` directly.

## Training a smaller model on a single GPU

To train a smaller Pythia 70M model on a single GPU, you can modify the `pretrain/openwebtext.py` file to use the following settings:


```python
model_name = "pythia-70m"
```

(Please see the the `download_*` scripts in the [../tutorials](../tutorials) for more information on downloading model checkpoints for different models.)

Also, before you start training, note that you will need to prepare the dataset specifically for this model since it may use a different tokenizer:

```bash
python scripts/prepare_openwebtext.py \
  --checkpoint_dir checkpoints/EleutherAI/pythia-70m/ \
  --destination_path data/lit-openwebtext

python pretrain/openwebtext.py \
  --devices 4
```

## Using the PyTorch Lightning `Trainer`

The `pretrain/openwebtext.py` used and discussed above uses Lightning Fabric, which is an open-source library for accessing more advanced PyTorch features conveniently (for example, mixed-precision training, multi-GPU training like FSDP, and more).

The PyTorch Lightning Trainer, which shares the same accelerator code with Fabric, offers additional features, such as more advanced checkpointing and logging. If you prefer using the PyTorch Lightning Trainer, you can use the alternative `pretrain/openwebtext_trainer.py` script:

```bash
python pretrain/openwebtext_trainer.py \
  --devices 4
```
## Download [Falcon](https://falconllm.tii.ae) weights

UAE's Technology Innovation Institute has open-sourced Falcon LLM.
It is trained on [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora
 Weights are released under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).

The first Falcon release includes a base model and an instruction tuned model of sizes 7B and 40B called `falcon-7b-instruct` and `falcon-40b-instruct`. Recently, checkpoints for 180B parameter models were added as well; the 180B instruction tuned model is called `falcon-180B-chat` and similar to the `falcon-40b-instruct` architecture except for its larger size.

To see all the available checkpoints for Falcon, run:

```bash
python scripts/download.py | grep falcon
```

which will print

```text
tiiuae/falcon-7b
tiiuae/falcon-7b-instruct
tiiuae/falcon-40b
tiiuae/falcon-40b-instruct
tiiuae/falcon-180B
tiiuae/falcon-180B-chat
```

In order to use a specific Falcon checkpoint, for instance [falcon-7b](https://huggingface.co/tiiuae/falcon-7b), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id tiiuae/falcon-7b

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/tiiuae/falcon-7b
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install tokenizers

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/tiiuae/falcon-7b
```

or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Lightning-AI/lit-gpt/blob/main/notebooks/falcon-inference.ipynb)
# Preparing Datasets

Below is a table of all datasets that are currently supported in Lit-GPT:


| Name         | Task        | Size                | Reference Repo                                                  | Paper / Blog                                                                                                              | Data License                                                                                                                                                                                                     |
|--------------|-------------|---------------------|-----------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Alpaca       | Finetuning  | 51,759 samples      | [URL](https://github.com/tatsu-lab/stanford_alpaca)             | [URL](https://crfm.stanford.edu/2023/03/13/alpaca.html)                                                                   | Attribution-NonCommercial 4.0 International, [ URL](https://crfm.stanford.edu/2023/03/13/alpaca.html)                                                                                                            |
| Alpaca Libre | Finetuning  | 55,370 samples      | [URL](https://github.com/mobarski/alpaca-libre)                 | -                                                                                                                         | CC0/MIT,  [URL](https://github.com/mobarski/alpaca-libre)                                                                                                                                                        |
| Dolly        | Finetuning  | 15,011 samples      | [URL](https://github.com/databrickslabs/dolly/tree/master/data) | [URL](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)              | CC-BY-SA, [URL](https://github.com/databrickslabs/dolly#model-overview)                                                                                                                                          |
| LongForm     | Finetuning  | 23,652 samples      | [URL](https://github.com/akoksal/LongForm)                      | [URL](https://arxiv.org/abs/2304.08460)                                                                                   | No information provided and subset-dependent, [URL](https://github.com/akoksal/LongForm) |
| LIMA         | Finetuning  | 1,084 samples       | [URL](https://huggingface.co/datasets/GAIR/lima)                | [URL](https://arxiv.org/abs/2305.11206)                                                                                   | "If the source data of LIMA has a stricter license than CC BY-NC-SA, the LIMA dataset follows the same. Otherwise, it follows the CC BY-NC-SA license", [URL](https://huggingface.co/datasets/GAIR/lima#license) |
| OpenWeb Text | Pretraining | 8,013,769 documents | [URL](https://github.com/jcpeterson/openwebtext)                | [URL](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | Unspecified                                                                                                                                                                                                      |
| RedPajama    | Pretraining | 1.2 T tokens        | [URL](https://github.com/togethercomputer/RedPajama-Data)       | [URL](https://together.ai/blog/redpajama-models-v1)                                                                       | Subset-dependent, [URL](https://github.com/togethercomputer/RedPajama-Data#license)                                                                                                                              |                                                                     |   |

&nbsp;

## Preparing Finetuning Datasets

Note that the dataset needs to be prepared separately for each type of model since the tokenizers used by the models may differ, resulting in slightly different preprocessed datasets.

For the following examples, we will use a Falcon 7B model. However, the same methods are compatible with all other models as well.

The steps here only need to be done once before preparing the finetuning datasets in the following subsections:

1. Follow the instructions in the [README](../README.md) to install the dependencies.
2. Download and convert the weights following our [guide](download_falcon.md).

&nbsp;

### Alpaca

&nbsp;


The Alpaca dataset consists of 52,000 instructions and demonstrations produced by OpenAI's text-davinci-003 engine. This data is used in instruction-tuning, helping improve the performance of language models to follow instructions.

In its development, the creators leveraged the data generation methodology from the [Self-Instruct framework](https://github.com/yizhongw/self-instruct).

The original [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) dataset can be prepared as follows:

```bash
python scripts/prepare_alpaca.py \
 --checkpoint_dir checkpoints/tiiuae/falcon-7b
```

#### Truncating datasets

By default, the finetuning script (for example [`finetuning/lora.py`](../finetuning/lora.py)) will determine the size of the longest tokenized sample in the dataset to determine the block size. However, if you are willing to truncate a few examples in the training set, you can reduce the computational resource requirements significantly. For instance you can set a sequence length threshold via `--max_seq_length`. We can determine an appropriate maximum sequence length by considering the distribution of the data sample lengths shown in the histogram below.

<img src="images/prepare_dataset/alpaca.jpg" width=400px>

In this case, a cut-off of 256 may be a reasonable choice:

```bash
python scripts/prepare_alpaca.py \
 --checkpoint_dir checkpoints/tiiuae/falcon-7b \
 --max_seq_length 256
```

For comparison, the Falcon 7B model requires 23.52 GB of memory for the original Alpaca dataset and 15.73 GB of memory for the truncated Alpaca dataset when finetuning with LoRA using a micro batchsize of 1 and bfloat-16 precision.

&nbsp;

### Alpaca Libre

[Alpaca Libre](https://github.com/mobarski/alpaca-libre) is a reimplementation or alternative to Alpaca using the same formatting.

To use Alpaca Libre instead of the original Alpaca dataset, use the following command:

```bash
python scripts/prepare_alpaca.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --data_file_url "https://raw.githubusercontent.com/mobarski/alpaca-libre/main/data/output/alpaca_libre_ok_tasks_v4.json" \
 --data_file_name "alpaca_libre_data_cleaned_archive.json" \
 --destination_path "data/alpaca_libre"
```

The Alpaca Libre dataset distribution is shown below.

<img src="images/prepare_dataset/alpaca_libre.jpg" width=400px>

You may want to consider truncating the dataset (see the *Truncating datasets* discussion in the Alpaca section for more information.) For this dataset, a cut-off of 256 may be a good choice:

```bash
python scripts/prepare_alpaca.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --data_file_url "https://raw.githubusercontent.com/mobarski/alpaca-libre/main/data/output/alpaca_libre_ok_tasks_v4.json" \
 --data_file_name "alpaca_libre_data_cleaned_archive.json" \
 --destination_path "data/alpaca_libre" \
 --max_seq_length 256
```



&nbsp;

### Dolly

The Dolly dataset is a publicly available collection of 15k instruction-following entries created by Databricks. It spans multiple behavioral domains, as described in the [InstructGPT paper](https://arxiv.org/abs/2203.02155) paper. These include areas like brainstorming, classification, closed QA, content creation, information retrieval, open QA, and summary generation.

The usage is similar to the Alpaca dataset described above. Using Falcon 7b as an example, we can prepare the dataset as follows:

```bash
python scripts/prepare_dolly.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b"
```

The Dolly dataset distribution is shown below.

<img src="images/prepare_dataset/dolly.jpg" width=400px>

You may want to consider truncating the dataset (see the *Truncating datasets* discussion in the Alpaca section for more information.) For this dataset, a cut-off of 512 may be a good choice:

```bash
python scripts/prepare_dolly.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --max_seq_length 512
```

&nbsp;

### LongForm

LongForm is a semi-synthetic dataset based on raw text corpora for which the instructions were generated via an LLM. For more details about the instruction-generation process, please refer to the [LongForm research paper](https://arxiv.org/abs/2304.08460) by Köksal et al. According to the research paper, a Llama 7B model trained on LongForm achieves substantially better performance than the same Llama model trained on the 2x larger Alpaca dataset.

LongForm consists of 23,652 training samples, 2,042 validation samples, and 2,045 test samples. (In Lit-GPT, the validation samples are currently not used.)

The more detailed dataset composition is as follows based on a table taken from the [dataset repository](https://github.com/akoksal/LongForm):

| **Type**               | **Source**     | **Number of Examples** |
|------------------------|----------------|------------------------|
| **Corpora**            | C4             | 10,000                 |
|                        | Wikipedia      | 5,000                  |
| **Structured Corpora** | Stack Exchange | 4,380                  |
|                        | WikiHow        | 2,500                  |
| **Tasks**              | NIv2           | 3,684                  |
|                        | Big Bench      | 600                    |
|                        | BEA-GEC        | 1,203                  |
|                        | Enron          | 372                    |
| **Total**              |                | 27,739                 |
|  |   |  |
| **Train**              |                | 23,652                 |
| **Validation**         |                | 2,042                  |
| **Test**               |                | 2,045                  |

License information is not provided but would depend on the individual subsets listed above.

The LongForm dataset distribution is shown below.

<img src="images/prepare_dataset/longform.jpg" width=400px>

You may want to consider truncating the dataset (see the *Truncating datasets* discussion in the Alpaca section for more information.) For this dataset, a cut-off of 1500 may be a good choice:

```bash
python scripts/prepare_dolly.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --max_seq_length 1500
```

&nbsp;

### LIMA

The LIMA dataset is a collection of 1,000 carefully curated prompts and responses, as described in the [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) paper. The dataset is sourced from three community Q&A websites: Stack Exchange, wikiHow, and the Pushshift Reddit Dataset. In addition, it also contains prompts and answers written and collected by the authors of the LIMA paper.

The usage is similar to the Dolly dataset described above except that it requires an Hugging Face access token that you need to copy & paste from your Hugging Face account. Using Falcon 7b as an example, we can prepare the dataset as follows:

```bash
python scripts/prepare_lima.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --access_token "insert_your_token_here"
```

LIMA contains a handful of multiturn conversations. By default, only the first instruction-response pairs from
each of these multiturn conversations are included. If you want to override this behavior and include the follow up instructions
and responses, set `--include_multiturn_conversations True`.

The Lima dataset distribution is shown below.

<img src="images/prepare_dataset/lima.jpg" width=400px>

You may want to consider truncating the dataset (see the *Truncating datasets* discussion in the Alpaca section for more information.) For this dataset, a cut-off of 512 may be a good choice:

```bash
python scripts/prepare_dolly.py \
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --max_seq_length 512
```


&nbsp;

### Finetuning After Data Preparation

After preparing the dataset, you can finetune the model using the [`finetune/*.py`](../finetune/) scripts, for example,

```bash
python finetune/lora.py
 --checkpoint_dir "checkpoints/tiiuae/falcon-7b" \
 --data_dir "data/alpaca_libre" \
 --out_dir "out/lora/alpaca"
```

Please read the [tutorials/finetune_*.md](../tutorials) documents for more information about finetuning models.

> [!IMPORTANT]
> Make sure that the `prepare_*.py` and `finetune/*.py` scripts use the same model checkpoint specified via `--checkpoint_dir`.

> [!IMPORTANT]
> By default, the maximum sequence length is obtained from the model configuration file. In case you run into out-of-memory errors, especially in the cases of LIMA and Dolly,
> you can try to lower the context length by preparing the dataset with a fixed max length, for example, `python scripts/prepare_lima.py --max_seq_length 2048`. For more information on truncating datasets, see the *Truncating datasets* section in the Alpaca section near the top of this article.

&nbsp;

## Preparing Custom Datasets for Instruction Finetuning

The models in Lit-GPT expect datasets for instruction finetuning in the following format:

```
[
    {
        "instruction": "Write a limerick about a
                        pelican.”,
        "input": "",
        "output": "There once was a pelican so fine,
                   \nHis beak was as colorful as
                   sunshine,\nHe would fish all day,\nIn
                   a very unique way,\nThis pelican was
                   truly divine!\n\n\n"
    },
    {
        "instruction": "Identify the odd one out from
                        the group.",
        "input": "Carrot, Apple, Banana, Grape",
        "output": "Carrot\n\n"
    },
]
```
(Note that depending on the task, the `"input"` text can be an empty string, as shown above.)

Custom datasets can be prepared by either creating a new `scripts/prepare_dataset.py` script or reading the dataset
from a CSV file.

&nbsp;

### Preparing Custom Datasets From a CSV File

You can prepare custom dataset using a CSV file with the following columns:

- `instruction`: Column which will describe the task.
- `input`: A string holding a special input value for the instruction. This applies to some samples, and in others, this is empty (empty string).
- `output`: The expected response

> If any of the columns is missing, then the script will fail to create the dataset.

Before you finetune, prepare the dataset using the `prepare_csv.py` script:

```bash
python scripts/prepare_csv.py --csv_path path/to/the/file.csv
```
You can also customize the dataset generation by using these additional parameters

- `destination_path`: The folder where the binary data will be saved. By default, it is saved inside `data/csv`

- `checkpoint_dir`: The model checkpoint dir. It will use the model's tokenizer to load and convert the string to input ids. Defaults to `"checkpoints/stabilityai/stablelm-base-alpha-3b"`

- `test_split_fraction`: The fraction of the data to split. Defaults to `0.1`

- `seed`: The seed value to reproduce the same random splits for train and test data.

- `mask_inputs`: Whether we require any masking or not.

- `ignore_index`: Mask out all the tokens after this index when preparing the dataset.

To use the the settings described above, you can add the respective command line arguments when calling `prepare_csv.py` as shown in the example below:

```bash
python scripts/prepare_csv.py --csv_path test_data.csv \
--destination_path data/csv \
--checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b \
--test_split_fraction 0.1 \
--seed 42 \
--mask_inputs false \
--ignore_index -1
```
Replace `test_data.csv` with your CSV path and the other additional parameters accordingly. Executing the command above will save `train.pt` and `test.pt` on your disk at the `destination_path`. Now you can use the prepared data to [finetune your model](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_lora.md#running-the-finetuning).

&nbsp;

### Preparing Custom Datasets Using a Dataset Prepration Script

If you don't have a CSV file following the format described in the previous section, the easiest way to prepare a new dataset is to copy and modify one of the existing dataset preparation scripts:

- [`scripts/prepare_alpaca.py`](https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/prepare_alpaca.py) (if you plan to load a dataset from a JSON file);
- [`scripts/prepare_lima.py`](https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/prepare_lima.py) (if you plan to load a dataset using the `datasets` Python library).

These scripts may look intimidating at first glance since they include code for tokenizing the dataset for a specific LLM that is provided via a checkpoint directory. However, note that you only need to modify a small fraction of the code file, namely the portion that downloads and formats the training data.

In [`scripts/prepare_lima.py`](https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/prepare_lima.py), the [line 26](https://github.com/Lightning-AI/lit-gpt/blob/98fad263a62e5e57821de817bdd5e316abfb34d4/scripts/prepare_lima.py#L26) references the HF repo ID, and the lines [50-53](https://github.com/Lightning-AI/lit-gpt/blob/98fad263a62e5e57821de817bdd5e316abfb34d4/scripts/prepare_lima.py#L50-L53) save the dataset as `train_data`. Here, `train_data` is a list that contains the instruction examples in the format mentioned above.


In [`scripts/prepare_alpaca.py`](https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/prepare_alpaca.py), you only need to modify [lines 24-25](https://github.com/Lightning-AI/lit-gpt/blob/98fad263a62e5e57821de817bdd5e316abfb34d4/scripts/prepare_alpaca.py#L24-L25) for the file name and URL, assuming the JSON file you are working with has the same format as the [Alpaca JSON file](https://raw.githubusercontent.com/tloen/alpaca-lora/main/alpaca_data_cleaned_archive.json).



&nbsp;

## Preparing Pretraining Datasets

In addition to the finetuning dataset described above, Lit-GPT also supports several datasets for pretraining. The pretraining datasets are described in more detail in the following separate tutorial documents:

- [Pretrain Llama 2 on OpenWebText](./pretrain_openwebtext.md)
- [Pretrain Llama 2 on RedPajama](./pretrain_redpajama.md)## Download TinyLlama weights

[TinyLlama 1.1B](https://github.com/jzhang38/TinyLlama/) is Apache 2.0 licensed and can be used without restrictions.
It is still in development and at the time of writing this, checkpoints for the model trained up to 1T tokens are available.
The target is to train it for ~3 epochs on 3T tokens total. For more details on the schedule and progress of the pretraining, see the official [README](https://github.com/jzhang38/TinyLlama/tree/main).


In order to use the TinyLLama 1.1B model checkpoint, which requires about 5 GB of disk space, download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id PY007/TinyLlama-1.1B-intermediate-step-480k-1T

python scripts/convert_hf_checkpoint.py \
    --checkpoint_dir checkpoints/PY007/TinyLlama-1.1B-intermediate-step-480k-1T
```

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/PY007/TinyLlama-1.1B-intermediate-step-480k-1T
```
## Download [StableLM](https://github.com/Stability-AI/StableLM) weights

StableLM is a family of generative language models trained by StabilityAI, trained on a dataset derived from [The Pile](https://pile.eleuther.ai/) but 3x larger, for a total of 1.5 trillion tokens. Weights are released under the [CC-BY-SA license](https://creativecommons.org/licenses/by-sa/4.0).

For more info on the models, please see the [StableLM repository](https://github.com/EleutherAI/pythia). 3B and a 7B checkpoints have been released, both after pre-training and after instruction tuning, using a combination of Stanford's Alpaca, Nomic-AI's gpt4all, RyokoAI's ShareGPT52K datasets, Databricks labs' Dolly, and Anthropic's HH.

To see all the available checkpoints for StableLM, run:

```bash
python scripts/download.py | grep stablelm
```

which will print

```text
stabilityai/stablelm-base-alpha-3b
stabilityai/stablelm-base-alpha-7b
stabilityai/stablelm-tuned-alpha-3b
stabilityai/stablelm-tuned-alpha-7b
```

In order to use a specific StableLM checkpoint, for instance [stablelm-base-alpha-3b](http://huggingface.co/stabilityai/stablelm-base-alpha-3b), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id stabilityai/stablelm-base-alpha-3b

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install tokenizers

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```
## Download [LongChat](https://lmsys.org/blog/2023-06-29-longchat) weights

LongChat is an open-source family of chatbots based on LLaMA featuring an extended context length up to 16K tokens.
The technique used to extend the context length is described in [this blogpost](https://kaiokendev.github.io/context).

To see all the available checkpoints, run:

```bash
python scripts/download.py | grep longchat
```

which will print

```text
lmsys/longchat-7b-16k
lmsys/longchat-13b-16k
```

In order to use a specific checkpoint, for instance [longchat-7b-16k](https://huggingface.co/lmsys/longchat-7b-16k), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id lmsys/longchat-7b-16k

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/lmsys/longchat-7b-16k
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/lmsys/longchat-7b-16k
```
## Download [Pythia](https://github.com/EleutherAI/pythia) weights

EleutherAI's project Pythia combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers. Weights are released under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).

For detailed info on the models, their training, and their behavior, please see the [Pythia repository](https://github.com/EleutherAI/pythia).
It includes a suite of 8 checkpoints (weights) on 2 different datasets: [The Pile](https://pile.eleuther.ai/), as well as The Pile with deduplication applied.

To see all the available checkpoints for Pythia, run:

```bash
python scripts/download.py | grep pythia
```

which will print

```text
EleutherAI/pythia-70m
EleutherAI/pythia-160m
EleutherAI/pythia-410m
EleutherAI/pythia-1b
EleutherAI/pythia-1.4b
EleutherAI/pythia-2.8b
EleutherAI/pythia-6.9b
EleutherAI/pythia-12b
EleutherAI/pythia-70m-deduped
EleutherAI/pythia-160m-deduped
EleutherAI/pythia-410m-deduped
EleutherAI/pythia-1b-deduped
EleutherAI/pythia-1.4b-deduped
EleutherAI/pythia-2.8b-deduped
EleutherAI/pythia-6.9b-deduped
EleutherAI/pythia-12b-deduped
```

In order to use a specific Pythia checkpoint, for instance [pythia-1b](https://huggingface.co/EleutherAI/pythia-1b), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id EleutherAI/pythia-1b

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/EleutherAI/pythia-1b
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install tokenizers

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/EleutherAI/pythia-1b
```
# Resource Tables

- Last updated: 10/20/2023 
- Lit-GPT version: commit 8641822
- Hardware: NVIDIA A100-SXM4-40GB
- OS: Ubuntu 22.04.3 LTS (x86_64)
- Nvidia driver version: 525.125.06
- Relevant libraries
  - CMake 3.26.4
  - Libc glibc-2.35 
  - PyTorch 2.1.0+cu121
  - Lightning 2.1.0.rc0
  - Bitsandbytes 0.41.1

This document provides an overview and examples of hardware requirements when running models in Lit-GPT.

For additional tips on lowering the GPU memory footprint, please also see the [Dealing with out-of-memory (OOM) errors](oom.md) document.

All experiments were run using 16-bit brain floating point precision (`--precision bf16-true`). If your GPU does not support brain floating point precision, you can use regular 16-bit floating point precision (`--precision 16-true`).

All experiments were conducted using the Alpaca dataset with its default length. Note that due to different tokenizers being used by the different models, the number of tokens in the longest training example differs based on the model:

- phi1.5: 1044 tokens
- StableLM Alpha: 1034 tokens
- Llama 2: 1304 tokens
- Falcon 1079 tokens

Note that the number of tokens in the training set does not affect the supported context width (block size) of the models, which is as follows:

- phi1.5: 2048 tokens
- StableLM 3B Alpha: 4096 tokens
- Llama 2: 4048 tokens
- Falcon: 2048 tokens
- CodeLlama 13B: 16384 tokens

&nbsp;

## Finetuning with LoRA on 1 GPU

The following experiments were conducated on 1xA100 with a minibatch size of 128 using the `finetune/lora.py` script.

| Size  | Model         | Quantization | Microbatch size | Trainable parameters | Max GPU RAM | Time 1k iterations | Time 50k iter (extrapolated) |
| ----- | ------------- | ------------ | --------------- | -------------------- | ----------- | ------------------ | ---------------------------- |
| 1.3 B | phi-1.5       | None         | 1               | 1,572,864            | 4.82 GB     | 1.62 min           | 80.91 min                    |
| 1.3 B | phi-1.5       | bnb.nf4      | 1               | 1,572,864            | 3.78 GB     | 1.77 min           | 88.36 min                    |
| 1.3 B | phi-1.5       | bnb.nf4-dq   | 1               | 1,572,864            | 3.72 GB     | 1.87 min           | 93.39 min                    |
| 1.3 B | phi-1.5       | None         | 2               | 1,572,864            | 6.76 GB     | 1.65 min           | 82.44 min                    |
| 1.3 B | phi-1.5       | None         | 4               | 1,572,864            | 10.68 GB    | 1.70 min           | 84.79 min                    |
|       |               |              |                 |                      |             |                    |                              |
| 3 B   | StableLM Alpha | None         | 1               | 2,097,152            | 9.69 GB     | 1.24 min           | 62.23 min                    |
| 3 B   | StableLM Alpha | bnb.nf4      | 1               | 2,097,152            | 6.35 GB     | 1.82 min           | 91.22 min                    |
| 3 B   | StableLM Alpha | bnb.nf4-dq   | 1               | 2,097,152            | 6.19 GB     | 1.87 min           | 93.58 min                    |
| 3 B   | StableLM Alpha | None         | 2               | 2,097,152            | 12.10 GB    | 1.33 min           | 66.68 min                    |
| 3 B   | StableLM Alpha | None         | 4               | 2,097,152            | 16.92 GB    | 1.50 min           | 74.89 min                    |
|       |               |              |                 |                      |             |                    |                              |
| 7 B   | Llama 2       | None         | 1               | 4,194,304            | 21.30 GB    | 2.36 min           | 118.03 min                   |
| 7 B   | Llama 2       | bnb.nf4      | 1               | 4,194,304            | 14.14 GB    | 3.68 min           | 183.88 min                   |
| 7 B   | Llama 2       | bnb.nf4-dq   | 1               | 4,194,304            | 13.84 GB    | 3.83 min           | 191.66 min                   |
| 7 B   | Llama 2       | None         | 2               | 4,194,304            | 29.07 GB    | 2.52 min           | 125.97 min                   |
| 7 B   | Llama 2       | None         | 4               | 4,194,304            | OOM         | -                  | -                            |
|       |               |              |                 |                      |             |                    |                              |
| 13 B  | Llama 2       | None         | 1               | 6,553,600            | 38.12 GB    | 3.19 min           | 159.43 min                   |
| 13 B  | Llama 2       | bnb.nf4      | 1               | 6,553,600            | 23.14 GB    | 6.38 min           | 319.03 min                   |
| 13 B  | Llama 2       | bnb.nf4-dq   | 1               | 6,553,600            | 22.55 GB    | 6.55 min           | 327.32 min                   |
| 13 B  | Llama 2       | None         | 2               | 6,553,600            | OOM         | -                  | -                            |
| 13 B  | Llama 2       | None         | 4               | 6,553,600            | OOM         | -                  | -                            |
|       |               |              |                 |                      |             |                    |                              |
| 40 B  | Falcon        | None         | 1               | 12,042,240           | OOM         | -                  | -                            |
| 40 B  | Falcon        | bnb.nf4      | 1               | 12,042,240           | OOM         | -                  | -                            |
| 40 B  | Falcon        | bnb.nf4-dq   | 1               | 12,042,240           | OOM         | -                  | -                            |

&nbsp;

## Finetuning with LoRA on Multiple GPUs

The following experiments were conducated on multiple A100 GPUs with a minibatch size of 128 using the `finetune/lora.py` script.

| Size  | Model         | Quantization | Microbatch size | Trainable parameters | GPU      | Max GPU RAM | Time 1k iterations | Time 50k iter (extrapolated) |
| ----- | ------------- | ------------ | --------------- | -------------------- | -------- | ----------- | ------------------ | ---------------------------- |
| 1.3 B | phi-1.5       | None         | 1               | 1,572,864            | 2 x A100 | 4.86 GB     | 3.81 min           | 190.47 min                   |
| 1.3 B | phi-1.5       | bnb.nf4      | 1               | 1,572,864            | 2 x A100 | N/A         | -                  | -                            |
| 1.3 B | phi-1.5       | bnb.nf4-dq   | 1               | 1,572,864            | 2 x A100 | N/A         | -                  | -                            |
| 1.3 B | phi-1.5       | None         | 2               | 1,572,864            | 2 x A100 | 5.05 GB     | 3.63 min           | 181.31 min                   |
| 1.3 B | phi-1.5       | None         | 4               | 1,572,864            | 2 x A100 | 5.88 GB     | 3.64 min           | 181.76 min                   |
|       |               |              |                 |                      |          |             |                    |                              |
| 3 B   | StableLM Alpha | None         | 1               | 2,097,152            | 2 x A100 | 12.75 GB    | 2.92 min           | 145.96 min                   |
| 3 B   | StableLM Alpha | None         | 2               | 2,097,152            | 2 x A100 | 12.94 GB    | 3.06 min           | 153.10 min                   |
| 3 B   | StableLM Alpha | None         | 4               | 2,097,152            | 2 x A100 | 13.45 GB    | 3.86 min           | 192.99 min                   |
|       |               |              |                 |                      |          |             | -                  | -                            |
| 7 B   | Llama 2       | None         | 1               | 4,194,304            | 2 x A100 | 22.18 GB    | 5.93 min           | 296.62 min                   |
| 7 B   | Llama 2       | None         | 2               | 4,194,304            | 2 x A100 | 22.47 GB    | 6.48 min           | 324.03 min                   |
| 7 B   | Llama 2       | None         | 4               | 4,194,304            | 2 x A100 | 23.39 GB    | 8.66 min           | 432.82 min                   |
|       |               |              |                 |                      |          |             |                    |                              |
| 13 B  | Llama 2       | None         | 1               | 6,553,600            | 2 x A100 | OOM         | -                  | -                            |
| 13 B  | Llama 2       | bnb.nf4      | 1               | 6,553,600            | 2 x A100 | N/A         | -                  | -                            |
| 13 B  | Llama 2       | bnb.nf4-dq   | 1               | 6,553,600            | 2 x A100 | N/A         | -                  | -                            |
|       |               |              |                 |                      |          |             |                    |                              |
| 13 B  | Llama 2       | None         | 1               | 6,553,600            | 4 x A100 | 35.57 GB    | 10.25 min          | 512.5 min                    |
| 40 B  | Falcon        | None         | 1               | 12,042,240           | 4 x A100 | OOM         | -                  | -                            |

&nbsp;

## Single-GPU Inference

| Size  | Model          | Quantization | GPU      | Max GPU RAM                                | Token/sec |
|-------|----------------|--------------|----------|--------------------------------------------|-----------|
| 1.3 B | phi-1.5        | None         | 1 x A100 | 2.86 GB                                   | 42.56     |
| 1.3 B | phi-1.5        | bnb.nf4      | 1 x A100 | 1.39 GB                                   | 22.89     |
| 1.3 B | phi-1.5        | bnb.nf4-dq   | 1 x A100 | 1.33 GB                                   | 22.75     |
| 1.3 B | phi-1.5        | gptq.int4    | 1 x A100 | 1.16 GB                                   | 6.51      |
|       |                |              |          |                                           |           |
| 3 B   | StableLM Alpha | None         | 1 x A100 | 7.30 GB                                   | 49.01     |
| 3 B   | StableLM Alpha | bnb.nf4      | 1 x A100 | 3.20 GB                                   | 29.04     |
| 3 B   | StableLM Alpha | bnb.nf4-dq   | 1 x A100 | 3.04 GB                                   | 27.15     |
| 3 B   | StableLM Alpha | gptq.int4    | 1 x A100 | 2.43 GB                                   | 5.9       |
|       |               |              |          |                                           |           |
| 7 B   | Llama 2       | None         | 1 x A100 | 13.52 GB                                  | 30.97     |
| 7 B   | Llama 2       | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |
| 7 B   | Llama 2       | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |
| 7 B   | Llama 2       | gptq.int4    | 1 x A100 | 3.93 GB                                   | 5.04      |
|       |               |              |          |                                           |           |
| 13 B  | Llama 2       | None         | 1 x A100 | 26.21 GB                                  | 24.82     |
| 13 B  | Llama 2       | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |
| 13 B  | Llama 2       | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |
| 13 B  | Llama 2       | gptq.int4    | 1 x A100 | 7.14 GB                                   | 4.17      |
|       |               |              |          |                                           |           |
| 34 B  | CodeLlama     | None         | 1 x A100 | OOM                                       | -         |
| 34 B  | CodeLlama     | bnb.nf4      | 1 x A100 | 20.52 GB                                  | 14.32     |
| 34 B  | CodeLlama     | bnb.nf4-dq   | 1 x A100 | 18.95 GB                                  | 12.37     |
| 34 B  | CodeLlama     | gptq.int4    | 1 x A100 | OOM (quantize script)                     | -         |
|       |               |              |          |                                           |           |
| 40 B  | Falcon        | None         | 1 x A100 | OOM                                       | -         |
| 40 B  | Falcon        | bnb.nf4      | 1 x A100 | 26.55 GB                                  | 13.25     |
| 40 B  | Falcon        | bnb.nf4-dq   | 1 x A100 | 24.63 GB                                  | 11.64     |
| 40 B  | Falcon        | gptq.int4    | 1 x A100 | OOM (quantize script)                     | -         |
|       |               |              |          |                                           |           |
| 70 B  | Llama 2       | None         | 1 x A100 | OOM                                       | -         |
| 70 B  | Llama 2       | bnb.nf4      | 1 x A100 | CUDA error: CUBLAS_STATUS_NOT_INITIALIZED | -         |
| 70 B  | Llama 2       | bnb.nf4-dq   | 1 x A100 | 37.21 GB                                  | 7.97      |
| 70 B  | Llama 2       | gptq.int4    | 1 x A100 | OOM (quantize script)                     | -         |
# Finetuning with Adapter

Adapter, first introduced for the LLaMA model as [LLaMA-Adapter](https://arxiv.org/abs/2303.16199), is a form of prefix-tuning that prepends a learnable adaption-prompt to the inputs of the attention blocks in an LLM. In total, there are only ~500k parameters to update during finetuning in StableLM 3B, which significantly reduces the memory footprint and speeds up training.

We are able to demonstrate instruction-finetuning Lit-GPT StableLM 3B on the [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset on a **single RTX 3060 GPU**. If using 8 GPUs, finetuning can be completed in under 1 hour.

If you are new to Adapter and are interested to learn more about how it works before proceeding with the finetuning guide below, you might find our article [Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/) helpful.

LLaMA-Adapter v2 extends the original LLaMA-Adapter idea by adding trainable bias and scale parameters to each linear layer in the transformer. Furthermore, LLaMA-Adapter v2 makes the normalization layers trainable. Where the StableLM 3B model has 500k trainable parameters with GPT v1, GPT-Adapter v2 adds an additional 1.5 M trainable parameter for the bias and scale parameters and ~300k trainable parameters for the normalization layers. So, adapter v2 has ~2.3 M trainable parameters in total.

## Preparation

The steps here only need to be done once:

1. Follow the instructions in the [README](../README.md) to install the dependencies.
2. Download and convert the weights following our [guide](download_stablelm.md).
3. Download the data and generate the Alpaca instruction tuning dataset:

```bash
python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

or [prepare your own dataset](#tune-on-your-dataset).

For more information about dataset preparation, also see the [prepare_dataset.md](./prepare_dataset.md) tutorial.

## Running the finetuning

```bash
python finetune/adapter.py --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

or for Adapter V2

```bash
python finetune/adapter_v2.py --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

The finetuning requires at least one GPU with ~12 GB memory.
You can speed up training by setting the `devices` variable in the script to utilize more GPUs if available.
Depending on the available GPU memory, you can also tune the `micro_batch_size` parameter to utilize the GPU efficiently.
To fit Adapter V2 to 12GB memory set micro_batch_size = 2.

For example, the following settings will let you finetune the model in under 1 hour:

```python
devices = 4
micro_batch_size = 4
```

This script will save checkpoints periodically to the `out_dir` directory. If you are finetuning different models or on your own dataset, you can specify an output directory with your preferred name:

```bash
python finetune/adapter.py --out_dir out/adapter/my-model-finetuned
```

or for Adapter V2

```bash
python finetune/adapter_v2.py --out_dir out/adapter_v2/my-model-finetuned
```

If your GPU does not support `bfloat16`, you can pass the `--precision 32-true` argument.
For instance, to fine-tune on MPS (the GPU on modern Macs), you can run

```bash
python finetune/adapter.py --out_dir out/adapter/my-model-finetuned --precision 32-true
```

Note that `mps` as the accelerator will be picked up automatically by Fabric when running on a modern Mac.

## Test the model

You can test the finetuned model with your own instructions by running:

```bash
python generate/adapter.py \
    --prompt "Recommend a movie to watch on the weekend." \
    --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

or for Adapter V2

```bash
python generate/adapter_v2.py \
    --prompt "Recommend a movie to watch on the weekend." \
    --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

Output:

```text
A good movie to watch on the weekend would be The Lion King, since it's a classic family film that everyone can enjoy...
```

If your GPU supports `bfloat16`, the script will automatically use it.

## Tune on your dataset

With only a few modifications, you can prepare and train on your own instruction dataset.

1. Create a json file in which each row holds one instruction-response pair.
   A row has an entry for 'instruction', 'input', and 'output', where 'input' is optional an can be
   the empty string if the instruction doesn't require a context. Below is an example json file:

    ```text
    [
        {
            "instruction": "Arrange the given numbers in ascending order.",
            "input": "2, 4, 0, 8, 3",
            "output": "0, 2, 3, 4, 8"
        },
        ...
    ]
    ```

2. Make a copy of `scripts/prepare_alpaca.py` and name it what you want:

    ```bash
    cp scripts/prepare_alpaca.py scripts/prepare_mydata.py
    ```

3. Modify `scripts/prepare_mydata.py` to read the json data file.
4. Run the script to generate the preprocessed, tokenized train-val split:

    ```bash
    python scripts/prepare_mydata.py --destination_path data/mydata/
    ```

5. Run `finetune/adapter.py` by passing in the location of your data (and optionally other parameters):

    ```bash
    python finetune/adapter.py \
        --data_dir data/mydata/ \
        --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b \
        --out_dir data/mydata-finetuned
    ```
# Pretrain Llama 2 on RedPajama

This tutorial will walk you through setting up the RedPajama dataset and launching the pretraining script.

## What's RedPajama

[RedPajama](https://github.com/togethercomputer/RedPajama-Data) is an open-source reproduction of the original LLaMA training dataset.

It contains a total of 1.2 trillion tokens, divided into

| Name          | Size |
|---------------|------|
| Commoncrawl   | 878B |
| C4            | 175B |
| GitHub        | 59B  |
| Books         | 26B  |
| ArXiv         | 28B  |
| Wikipedia     | 24B  |
| StackExchange | 20B  |

The [RedPajama repo](https://github.com/togethercomputer/RedPajama-Data) contains the source code for collecting and preparing the dataset, which is Apache 2.0 licensed.

The data itself is licensed according to the original licenses with which its individual parts were released.
The GitHub datasets are limited to MIT, BSD, or Apache 2.0 repositories.

Along with the full [RedPajama-1T dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T),
the smaller [RedPajama-1T-Sample](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample) 1B sample dataset is also available for development.

You can download the data using git lfs:

```bash
# Make sure you have git-lfs installed (https://git-lfs.com):
git lfs install
```

```bash
# The full 1 trillion token dataset:
git clone https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T data/RedPajama-Data-1T
```

```bash
# The 1 billion token subset
git clone https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample \
  data/RedPajama-Data-1T-Sample
```

## Prepare RedPajama for training

The full dataset consists of 2084 `jsonl` files (the sample dataset contains 11). In order to start pretraining lit-gpt
on it, you need to read, tokenize, and write the data in binary chunks. This will leverage the `PackedDataset`
streaming dataset that comes with lit-gpt. You will need to have the tokenizer config available:

```bash
pip install huggingface_hub sentencepiece

python scripts/download.py \
   --repo_id meta-llama/Llama-2-7b-chat-hf \
   --access_token your_hf_token
```

Then, run

```bash
python scripts/prepare_redpajama.py \
  --source_path data/RedPajama-Data-1T \
  --checkpoint_dir checkpoints/meta-llama/Llama-2-7b-hf/ \
  --destination_path data/lit-redpajama
```

or

```bash
python scripts/prepare_redpajama.py \
  --source_path data/RedPajama-Data-1T-Sample \
  --checkpoint_dir checkpoints/meta-llama/Llama-2-7b-hf/ \
  --destination_path data/lit-redpajama-sample \
  --sample True
```

for the sample dataset.

In the above we are assuming that you will be using the same tokenizer as used in LLaMA, but any trained [SentencePiece](https://github.com/google/sentencepiece) tokenizer with a 32000 vocabulary size will do here.

The script will take a while to run, so time for :tea: (The 1B sample script takes about 45 min for the data preparation.)

## Pretraining

Running the pretraining script with its default settings requires at least 4 GPUs with 40GB+ each (A100).

```bash
python pretrain/redpajama.py \
  --devices 4 \
  --train_data_dir data/lit-redpajama
```

For running on the sample dataset:

```bash
python pretrain/redpajama.py \
  --devices 4 \
  --train_data_dir data/lit-redpajama-sample
```

The script will save checkpoints periodically to the folder `out/`.

By default, the `pretrain/redpajama.py` script will pretrain the Llama 2 7B model with FSDP in
`bfloat16` precision and gradient accumulation.

You can easily change the size of the model by passing a different string to the model name variable

```python
model_name = "Llama-2-7b-hf"
```

at the top of this script.

The currently supported model names are contained in the [config.py](https://github.com/Lightning-AI/lit-gpt/lit_gpt/config.py) file.
You can

1) either search this file for lines containing "name =",
2) or run `python scripts/download.py` without additional command line arguments

Keep in mind that the original LLaMA training for the 7B model required 83k A100 80GB
hours, so you'll need access to a cluster.

Once you're in a cluster, you can follow [these instructions](https://lightning.ai/docs/fabric/stable/fundamentals/launch.html#launch-on-a-cluster)
to launch the script across machines:

- [SLURM cluster](https://lightning.ai/docs/fabric/stable/guide/multi_node/slurm.html)
- [Barebones cluster](https://lightning.ai/docs/fabric/stable/guide/multi_node/barebones.html)
- [MPI](https://lightning.ai/docs/fabric/stable/guide/multi_node/other.html)

The [script contains several configurations and hyperparameters](https://github.com/Lightning-AI/lit-gpt/blob/main/pretrain/openwebtext.py#L23-L46) you can tweak.

For instance, `micro_batch_size` should be adjusted so the process will use the available
GPU memory. For more tips to avoid out-of-memory issues, please also see the more detailed
[Dealing with out-of-memory (OOM) errors](oom.md) guide.

Last, logging is kept minimal in the script. In order to use a particular logger
please refer to <https://lightning.ai/docs/fabric/stable/api/loggers.html> or
call a logging client library like `wandb` directly.
## Download [phi-1.5](https://arxiv.org/abs/2309.05463) weights

A team at Microsoft Research has made available Phi 1.5, which is a 1.3 billion parameter model optimized for common sense reasoning in natural language, showing performance on par with models 5x its size, especially in grade-school mathematics and basic coding. This model retains characteristics of larger LLMs, and significant improvement was noted in reducing toxic and biased generations by avoiding web data. It's also worth highlighting that while this model performs well on language understanding and common sense reasoning tasks, it is a base model that has not undergone any supervised instruction finetuning or finetuning with RLHF.

The model was trained the same data sources (7B tokens) as its [phi-1](https://arxiv.org/abs/2306.11644) predecessor, which includes

- a Python code subset from [The Stack](https://arxiv.org/abs/2211.15533) v1.2
- Q&A texts from [StackOverflow](https://archive.org/download/stackexchange)
- code from DeepMind [code_contests](https://github.com/deepmind/code_contests)
- synthetic Python textbooks and exercises generated by [gpt-3.5-turbo-0301](https://platform.openai.com/docs/models/gpt-3-5)

In addition, to create phi-1.5, the authors included additional textbook-quality synthetic text (roughly 20B tokens) in natural language, which was created using the [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) approach.


The model weights are released under a [*Microsoft Research license*](https://huggingface.co/microsoft/phi-1_5/blob/main/README.md#license).


In order to use the phi-1.5 model checkpoint, which requires about 3 Gb of disk space, download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id microsoft/phi-1_5

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/microsoft/phi-1_5
```

You're done! To execute the model just run:

```bash
pip install tokenizers

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/microsoft/phi-1_5
```
## Download [OpenLLaMA](https://github.com/openlm-research/open_llama) weights

OpenLLaMA is a permissively licensed open source reproduction of [Meta AI’s LLaMA](https://github.com/facebookresearch/llama)
7B and 13B checkpoints trained on the [RedPajama dataset](https://github.com/togethercomputer/RedPajama-Data).
The weights can serve as the drop in replacement of LLaMA in existing implementations. We also provide a smaller 3B variant.

To see all the available checkpoints for Open LLaMA, run:

```bash
python scripts/download.py | grep open_llama
```

which will print

```text
openlm-research/open_llama_3b
openlm-research/open_llama_7b
openlm-research/open_llama_13b
```

In order to use a specific OpenLLaMA checkpoint, for instance [open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id openlm-research/open_llama_3b

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/openlm-research/open_llama_3b
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install sentencepiece

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/openlm-research/open_llama_3b
```
## Converting Lit-GPT weights to HuggingFace Transformers

Lit-GPT weights need to be converted to a format that HuggingFace understands with a [conversion script](../scripts/convert_lit_checkpoint.py) before our scripts can run.

We provide a helpful script to convert models Lit-GPT models back to their equivalent HuggingFace Transformers format:

```sh
python scripts/convert_lit_checkpoint.py \
    --checkpoint_path path/to/litgpt/model.pth \
    --output_path where/to/save/the/converted.ckpt \
    --config_path path/to/litgpt/config.json
```

These paths are just placeholders, you will need to customize them based on which finetuning or pretraining script you ran and it's configuration.


Please note that if you want to convert a model that has been fine-tuned using an adapter like LoRA, these weights should be [merged](../scripts/merge_lora.py) to the checkpoint prior to converting. 

```sh
python scripts/merge_lora.py \
    --checkpoint_dir path/to/litgpt/model.pth \
    --lora_path path/to/litgpt/lora_finetuned.pth \
    --out_dir where/to/save/the/merged.ckpt
```

## Download [FreeWilly 2](https://stability.ai/blog/freewilly-large-instruction-fine-tuned-models) weights

Stability AI announced FreeWilly inspired by the methodology pioneered by Microsoft in its paper: "Orca: Progressive Learning from Complex Explanation Traces of GPT-4”.
FreeWilly2 leverages the Llama 2 70B foundation model to reach a performance that compares favorably with GPT-3.5 for some tasks.

```bash
pip install huggingface_hub

python scripts/download.py --repo_id stabilityai/FreeWilly2

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/stabilityai/FreeWilly2
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/stabilityai/FreeWilly2
```
## Download [Mistral](https://mistral.ai) weights

[Mistral 7B](https://mistral.ai/news/announcing-mistral-7b) is Apache 2.0 licensed and can be used without restrictions. It:

* Outperforms Llama 2 13B on all benchmarks
* Outperforms Llama 1 34B on many benchmarks
* Approaches CodeLlama 7B performance on code, while remaining good at English tasks
* Uses Grouped-query attention (GQA) for faster inference
* ~~Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost~~.
  This project's implementation does not use Sliding Window Attention, so the context length is limited to 4096 tokens.

Details about the data used to train the model or training procedure have not been made public.

In order to use the Mistral 7B model checkpoint, which requires about 14 Gb of disk space, download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id mistralai/Mistral-7B-Instruct-v0.1

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.1
```

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.1
```
# Quantize the model

This document provides different strategies for quantizing the various models available in Lit-GPT to reduce GPU memory usage, which is useful for running larger models on certain GPU hardware.

**All the examples below were run on an A100 40GB GPU with CUDA 12.1.**

> [!NOTE]
> Quantization also supports finetuning via [QLoRA](finetune_lora.md)

## Baseline

It's useful to start with a baseline to have a reference point for memory savings via the various quantization methods.

```bash
python generate/base.py --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision 32-true --max_new_tokens 256
...
Time for inference 1: 6.93 sec total, 36.96 tokens/sec.
Memory used: 28.95 GB
```

First, using a lower precision compared to 32-bit float can result in two times reduced memory consumption. You can either try setting `--precision 16-true` for regular 16-bit precision or  `--precision bf16-true` if your GPU supports brain-float 16-bit precision. ([This brief video](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.1-accelerated-model-training-via-mixed-precision-training/) explains the difference between regular 16-bit and bf16-bit precision.)

In short, when `--precision bf16-true` or `--precision 16-true` is used, the model weights will automatically be converted and consume less memory.
However, this might not be enough for large models or when using GPUs with limited memory.

```bash
python generate/base.py --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256
...
Time for inference 1: 5.37 sec total, 47.66 tokens/sec.
Memory used: 14.50 GB
```

To reduce the memory requirements further, Lit-GPT supports several quantization techniques, which are shown below.

> [!NOTE]
> Most quantization examples below also use the `--precision bf16-true` setting explained above. If your GPU does not support `bfloat16`, you can change it to `--precision 16-true`.

## `bnb.nf4`

Enabled with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). Check out the [paper](https://arxiv.org/abs/2305.14314v1) to learn more about how it works.

> [!NOTE]
> `bitsandbytes` only supports `CUDA` devices and the `Linux` operating system.
> Windows users should use [WSL2](https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl).

Uses the normalized float 4 (nf4) data type. This is recommended over "fp4" based on the paper's experimental results and theoretical analysis.

```bash
pip install scipy bitsandbytes  # scipy is required until https://github.com/TimDettmers/bitsandbytes/pull/525 is released

python generate/base.py --quantize bnb.nf4 --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256
...
Time for inference 1: 6.80 sec total, 37.62 tokens/sec
Memory used: 5.72 GB
```

## `bnb.nf4-dq`

Enabled with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). Check out the [paper](https://arxiv.org/abs/2305.14314v1) to learn more about how it works.

"dq" stands for "Double Quantization" which reduces the average memory footprint by quantizing the quantization constants.
In average, this amounts to about 0.37 bits per parameter (approximately 3 GB for a 65B model).

```bash
pip install scipy bitsandbytes  # scipy is required until https://github.com/TimDettmers/bitsandbytes/pull/525 is released

python generate/base.py --quantize bnb.nf4-dq --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256
...
Time for inference 1: 8.09 sec total, 30.87 tokens/sec
Memory used: 5.38 GB
```

## `bnb.fp4`

Enabled with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). Check out the [paper](https://arxiv.org/abs/2305.14314v1) to learn more about how it works.

Uses pure FP4 quantization.

```bash
pip install scipy bitsandbytes  # scipy is required until https://github.com/TimDettmers/bitsandbytes/pull/525 is released

python generate/base.py --quantize bnb.fp4 --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256
...
Time for inference 1: 6.92 sec total, 36.98 tokens/sec
Memory used: 5.72 GB
```

## `bnb.fp4-dq`

Enabled with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). Check out the [paper](https://arxiv.org/abs/2305.14314v1) to learn more about how it works.

"dq" stands for "Double Quantization" which reduces the average memory footprint by quantizing the quantization constants.
In average, this amounts to about 0.37 bits per parameter (approximately 3 GB for a 65B model).

```bash
pip install scipy bitsandbytes  # scipy is required until https://github.com/TimDettmers/bitsandbytes/pull/525 is released

python generate/base.py --quantize bnb.fp4-dq --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision bf16-true --max_new_tokens 256
...
Time for inference 1: 10.02 sec total, 25.54 tokens/sec
Memory used: 5.38 GB
```

## `bnb.int8`

Enabled with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes). Check out the [paper](https://arxiv.org/abs/2110.02861) to learn more about how it works.

```bash
pip install scipy bitsandbytes  # scipy is required until https://github.com/TimDettmers/bitsandbytes/pull/525 is released

python generate/base.py --quantize bnb.int8 --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision 16-true --max_new_tokens 256
...
Time for inference 1: 20.22 sec total, 12.66 tokens/sec
Memory used: 8.70 GB
```

## `gptq.int4`

Check out the [paper](https://arxiv.org/abs/2210.17323) to learn more about how it works.

This technique needs a conversion of the weights first:

```bash
pip install datasets

python quantize/gptq.py --precision bf16-true --checkpoint_dir checkpoints/tiiuae/falcon-7b
...
Time for quantization: 850.25 sec total
Memory used: 23.68 GB
```

It is important to note that this conversion step required a considerable amount of memory (higher than regular inference) and may take a long time, depending on the size of the model.

generation then works as usual with `--quantize gptq.int4` which will load the newly quantized checkpoint file:

```bash
python generate/base.py --quantize gptq.int4 --checkpoint_dir checkpoints/tiiuae/falcon-7b --precision 32-true --max_new_tokens 256
...
Time for inference 1: 34.35 sec total, 7.45 tokens/sec
Memory used: 5.05 GB
```
## Download [RedPajama-INCITE](https://www.together.xyz/blog/redpajama-models-v1) weights

Togethercomputer's RedPajama-INCITE family of models were trained over the [RedPajama v1](https://www.together.xyz/blog/redpajama) dataset, with the same architecture as the popular [Pythia](download_pythia.md) model suite. Weights are released under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).

The release includes a base model, a chat fine-tuned model, and an instruction tuned model of sizes 3B and 7B.

To see all the available checkpoints for RedPajama-INCITE, run:

```bash
python scripts/download.py | grep RedPajama
```

which will print

```text
togethercomputer/RedPajama-INCITE-Base-3B-v1
togethercomputer/RedPajama-INCITE-Chat-3B-v1
togethercomputer/RedPajama-INCITE-Instruct-3B-v1
togethercomputer/RedPajama-INCITE-7B-Base
togethercomputer/RedPajama-INCITE-7B-Chat
togethercomputer/RedPajama-INCITE-7B-Instruct
togethercomputer/RedPajama-INCITE-Base-7B-v0.1
togethercomputer/RedPajama-INCITE-Chat-7B-v0.1
togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1
```

In order to use a specific RedPajama-INCITE checkpoint, for instance [RedPajama-INCITE-Base-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1), download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id togethercomputer/RedPajama-INCITE-Base-3B-v1

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/togethercomputer/RedPajama-INCITE-Base-3B-v1
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install tokenizers

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/togethercomputer/RedPajama-INCITE-Base-3B-v1
```
## Download [Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/) weights

Meta developed and publicly released the Code Llama family of large language models (LLMs) on top of Llama 2.

Code Llama models come in three sizes: 7B, 13B, and 34B parameter models. Furthermore, there are three model versions for each size:

- Code Llama: A base model trained on 500B tokens, then and finetuned on 20B tokens.
- Code Llama-Python: The Code Llama model pretrained on 500B tokens, further trained on 100B additional Python code tokens, and then finetuned on 20B tokens.
- Code Llama-Instruct: The Code Llama model trained on 500B tokens, finetuned on 20B tokens, and instruction-finetuned on additional 5B tokens.

All models were  trained on 16,000 token contexts and support generations with up to 100,000 tokens of context.

To see all the available checkpoints, run:

```bash
python scripts/download.py | grep CodeLlama
```

which will print

```text
codellama/CodeLlama-7b-hf
codellama/CodeLlama-7b-Python-hf
codellama/CodeLlama-7b-Instruct-hf
codellama/CodeLlama-13b-hf
codellama/CodeLlama-13b-Python-hf
codellama/CodeLlama-13b-Instruct-hf
codellama/CodeLlama-34b-hf
codellama/CodeLlama-34b-Python-hf
codellama/CodeLlama-34b-Instruct-hf
```

In order to use a specific checkpoint, for instance [CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf), download the weights and convert the checkpoint to the lit-gpt format.

```bash
pip install huggingface_hub

python scripts/download.py --repo_id codellama/CodeLlama-7b-Python-hf

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/codellama/CodeLlama-7b-Python-hf
```

By default, the `convert_hf_checkpoint.py` step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/codellama/CodeLlama-7b-Python-hf/
```
# LLM Evaluation

&nbsp;

## Using lm-evaluation-harness

You can evaluate Lit-GPT using [EleutherAI's lm-eval](https://github.com/EleutherAI/lm-evaluation-harness/tree/master) framework with a large number of different evaluation tasks.

You need to install the `lm-eval` framework first:

```bash
pip install https://github.com/EleutherAI/lm-evaluation-harness/archive/refs/heads/master.zip -U
```

&nbsp;

### Evaluating Lit-GPT base models

Use the following command to evaluate Lit-GPT models on all tasks in Eleuther AI's Evaluation Harness.

```bash
python eval/lm_eval_harness.py \
    --checkpoint_dir "checkpoints/meta-llama/Llama-2-7b-hf" \
    --precision "bf16-true" \
    --batch_size 4 \
    --save_filepath "results.json"
```

To evaluate on LLMs on specific tasks, for example, TruthfulQA and HellaSwag, you can use the `--eval_task` flag as follows:

```bash
python eval/lm_eval_harness.py \
    --checkpoint_dir "checkpoints/meta-llama/Llama-2-7b-hf" \
    --eval_tasks "[truthfulqa_mc,hellaswag]" \
    --precision "bf16-true" \
    --batch_size 4 \
    --save_filepath "results.json"
```

A list of supported tasks can be found [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).

&nbsp;

### Evaluating LoRA-finetuned LLMs

The above command can be used to evaluate models that are saved via a single checkpoint file. This includes downloaded checkpoints and base models finetuned via the full and adapter finetuning scripts.

For LoRA-finetuned models, you need to first merge the LoRA weights with the original checkpoint file as described in the [Merging LoRA Weights](https://github.com/Lightning-AI/lit-gpt/blob/main/tutorials/finetune_lora.md#merging-lora-weights) section of the LoRA finetuning documentation.

&nbsp;

## FAQs

* **How do I evaluate on MMLU?**

  MMLU is available as with lm-eval harness but the task name is not MMLU. You can use `hendrycksTest*` as regex to evaluate on MMLU.

  ```shell
  python eval/lm_eval_harness.py \
      --checkpoint_dir "checkpoints/meta-llama/Llama-2-7b-hf" \
      --precision "bf16-true" \
      --eval_tasks "[hendrycksTest*]" \
      --batch_size 4 \
      --num_fewshot 5 \
      --save_filepath "results.json"
  ```

* **Is Truthful MC is not available in lm-eval?**

  It is available as `truthfulqa_mc`.
## Download [Llama 2](https://ai.meta.com/llama) weights

Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and
fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Its fine-tuned LLMs,
called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on
most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular
closed-source models like ChatGPT and PaLM.

Llama 2 models are trained on 2 trillion tokens (40% more data than LLaMA 1) and have double the context length of LLaMA 1 (4096 tokens).

Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.

To see all the available checkpoints, run:

```bash
python scripts/download.py | grep Llama-2
```

which will print

```text
meta-llama/Llama-2-7b-hf
meta-llama/Llama-2-7b-chat-hf
meta-llama/Llama-2-13b-hf
meta-llama/Llama-2-13b-chat-hf
meta-llama/Llama-2-70b-hf
meta-llama/Llama-2-70b-chat-hf
```

In order to use a specific checkpoint, for instance [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), download the weights and convert the checkpoint to the lit-gpt format.

This requires that you've been granted access to the weights on the HuggingFace hub. You can do so by following the steps at <https://huggingface.co/meta-llama/Llama-2-7b>.
After access is granted, you can find your HF hub token in <https://huggingface.co/settings/tokens>.

```bash
pip install huggingface_hub

python scripts/download.py --repo_id meta-llama/Llama-2-7b-chat-hf --access_token your_hf_token

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/meta-llama/Llama-2-7b-chat-hf
```

By default, the convert_hf_checkpoint step will use the data type of the HF checkpoint's parameters. In cases where RAM
or disk size is constrained, it might be useful to pass `--dtype bfloat16` to convert all parameters into this smaller precision before continuing.

You're done! To execute the model just run:

```bash
pip install sentencepiece

python chat/base.py --checkpoint_dir checkpoints/meta-llama/Llama-2-7b-chat-hf
```
# Finetuning with LoRA / QLoRA

[Low-rank adaption (LoRA)](https://arxiv.org/abs/2106.09685) is a technique to approximate the update to the linear layers in a LLM with a low-rank matrix factorization. This significantly reduces the number of trainable parameters and speeds up training with little impact on the final performance of the model.
We demonstrate this method by instruction-finetuning Lit-GPT StableLM 3B on the [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset on a **single RTX 3090 (24GB) GPU** with CUDA 11.8.

&nbsp;

## Preparation

The steps here only need to be done once:

1. Follow the instructions in the [README](../README.md) to install the dependencies.
2. Download and convert the weights and save them in the `./checkpoints` folder.
   Weights can be downloaded following these instructions:

- [StableLM](download_stablelm.md)
- [Pythia](download_pythia.md)
- [Redpajama-INCITE](download_redpajama_incite.md)
- [Falcon](download_falcon.md)

3. Download the data and generate the instruction tuning dataset:

```bash
python scripts/prepare_alpaca.py \
  --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

or [prepare your own dataset](#tune-on-your-dataset).

For more information about dataset preparation, also see the [prepare_dataset.md](./prepare_dataset.md) tutorial.

&nbsp;

## Running the Finetuning

```bash
python finetune/lora.py
```

The finetuning requires at least one GPU with ~24 GB memory (RTX 3090).

This script will save checkpoints periodically to the folder `out/`.

> [!NOTE]
> LoRA can be applied to not only `query`, `key` or `value` matrices, but also to `projection`, `mlp` and classification `head`.
> According to [QLoRA](https://arxiv.org/abs/2305.14314) paper (section 4): "LoRA on all linear transformer block layers are required to match full finetuning performance".
> By default LoRA is applied only to the `query` and `value` matrices. In order to apply LoRA to other weight matrices - change the variables in `finetune/lora.py` accordingly.

Optionally, finetuning using 4-bit quantization (as in QLoRA) can be enabled via the `--quantize` flag, for example using the 4-bit NormalFloat data type:

```bash
python finetune/lora.py --quantize "bnb.nf4"
```

and optionally with double-quantization:

```bash
python finetune/lora.py --quantize "bnb.nf4-dq"
```

The table below lists a comparison with different settings on a StableLM 3B model finetuned with LoRA on Alpaca for 1,000 iterations using a microbatch size of 1:

| Settings                                    | Training Memory | Training Time |  Inference Memory |
|---------------------------------------------|-----------------|---------------|-------------------|
| Default (bf16-mixed)                        | 26.92 GB        | 1.34 min      | 21.43 GB          |
| --precision bf16-true                       | 9.69 GB         | 1.24 min      | 7.30 GB           |
| --precision bf16-true --quantize bnb.nf4    | 6.35 GB         | 1.82 min      | 3.20 GB           |
| --precision bf16-true --quantize bnb.nf4-dq | 6.19 GB         | 1.87 min      | 3.04 GB           |

The advantages of QLoRA-style quantization are more pronounced in larger models, such as Llama 2 7B. The table below summarizes the results for Llama 2 7B on Alpaca for 1,000 iterations using a microbatch size of 1:

| Settings                                    | Training Memory  | Training Time | Inference Memory |
|---------------------------------------------|------------------|---------------|------------------|
| Default (bf16-mixed)                        | OutOfMemoryError | N/A           | 40.21 GB         |
| --precision bf16-true                       | 21.30 GB         | 2.36 min      | 13.52 GB         |
| --precision bf16-true --quantize bnb.nf4    | 14.14 GB         | 3.68 min      | 4.57 GB          |
| --precision bf16-true --quantize bnb.nf4-dq | 13.84 GB         | 3.83 min      | 4.26 GB          |

For additional benchmarks and resource requirements, please see the [Resource Tables](resource-tables.md).

&nbsp;

## Test the Model

You can test the finetuned model with your own instructions by running:

```bash
python generate/lora.py \
  --prompt "Recommend a movie to watch on the weekend."
```

Output:

```text
I would recommend the movie The Martian (2015). It is a sci-fi movie starring Matt Damon that follows the story of...
```

If your GPU supports `bfloat16`, you can additionally pass `--precision "bf16-true"` to bring the memory consumption down to ~7.6 GB for StableLM-3B (versus ~15.2  GB for `--precision "32-full"`). In addition, you may use quantization methods, for example `--precision "bf16-true" --quantize "bnb.nf4"` brings the memory consumption further down to ~4.4 GB for StableLM-3B.

&nbsp;

## Tune on Your Dataset

With only a few modifications, you can prepare and train on your own instruction dataset.

1. Create a json file in which each row holds one instruction-response pair.
   A row has an entry for 'instruction', 'input', and 'output', where 'input' is optional an can be
   the empty string if the instruction doesn't require a context. Below is an example json file:

   ```text
   [
       {
           "instruction": "Arrange the given numbers in ascending order.",
           "input": "2, 4, 0, 8, 3",
           "output": "0, 2, 3, 4, 8"
       },
       ...
   ]
   ```

2. Make a copy of `scripts/prepare_alpaca.py` and name it what you want:

   ```bash
   cp scripts/prepare_alpaca.py scripts/prepare_mydata.py
   ```

3. Modify `scripts/prepare_mydata.py` to read the json data file.
4. Run the script to generate the preprocessed, tokenized train-val split:

   ```bash
   python scripts/prepare_mydata.py \
     --destination_path data/mydata/
   ```

5. Run `finetune/lora.py` by passing in the location of your data (and optionally other parameters):

   ```bash
   python finetune/lora.py  \
     --data_dir data/mydata/ \
     --out_dir out/myexperiment
   ```

&nbsp;

## Merging LoRA Weights

By default, the LoRA weights are kept separate from the checkpoint file to save storage space.
However, you can optionally merge the LoRA weights with the original model checkpoint to create
a new file to optimize inference speeds. (This will improve inference performance
because the weights don't have to be added during runtime.)

Let's assume we finetuned a model using LoRA as follows:

```bash
python finetune/lora.py \
  --checkpoint_dir "checkpoints/stabilityai/stablelm-base-alpha-3b/" \
  --data_dir "data/alpaca" \
  --out_dir "out/lora_weights/stablelm-base-alpha-3b/"
```

Then, we can merge the LoRA weights with the checkpoint model using the `merge_lora.py` script as shown below:

```bash
python scripts/merge_lora.py \
  --checkpoint_dir "checkpoints/stabilityai/stablelm-base-alpha-3b/" \
  --lora_path "out/lora_weights/stablelm-base-alpha-3b/lit_model_lora_finetuned.pth" \
  --out_dir "out/lora_merged/stablelm-base-alpha-3b/"
```

> [!Note]
> If you changed the LoRA hyperparameters (`lora_r`, `lora_key`, etc.) in the
> `finetune/lora.py` script, it is important to update the hyperparameter configuration
> in the `scripts/merge_lora.py` script accordingly. Otherwise, you will encounter size
> mismatch errors upon merging.

After merging, we can use the `base.py` file for inference using the new checkpoint file. Note that if your new checkpoint directory is different from the original checkpoint directory, we also have to copy over the tokenizer and config files:

```bash
cp checkpoints/stabilityai/stablelm-base-alpha-3b/*.json \
out/lora_merged/stablelm-base-alpha-3b/
```

> [!Note]
> Some models (for example, Llama 2) also come with a `tokenizer.model` file.
> In this case, you also need to use an additional copy step:
> `cp checkpoints/origin/tokenizer.model out/lora_merged/target/`

Then, we should be ready to use the model in inference:

```bash
python generate/base.py \
  --checkpoint_dir "out/lora_merged/stablelm-base-alpha-3b/"
```

Similarly, you can evaluate the model using the `eval/lm_eval_harness.py` script (see the [evaluation](evaluation.md) tutorial for more information):

```bash
python eval/lm_eval_harness.py \
  --checkpoint_dir "out/lora_merged/stablelm-base-alpha-3b/" \
  --precision "bf16-true" \
  --batch_size 4 \
  --save_filepath "results.json"
```
# Finetuning the whole model

If you are interested in parameter-efficient finetuning, check out [finetune_adapter.md](finetune_adapter.md). In contrast to parameter-efficient finetuning, this "full" approach finetunes all model parameters, which is substantially more expensive. It may only be recommended as a baseline for comparison studies.

## Preparation

The steps here only need to be done once:

1. Follow the instructions in the [README](../README.md) to install the dependencies.
2. Download and convert the weights following our [guide](download_stablelm.md).
3. Download the data and generate the Alpaca instruction tuning dataset:

```bash
python scripts/prepare_alpaca.py --checkpoint_dir checkpoints/tiiuae/falcon-7b
```

or [prepare your own dataset](#tune-on-your-dataset).

For more information about dataset preparation, also see the [prepare_dataset.md](./prepare_dataset.md) tutorial.

## Running the finetuning

```bash
python finetune/full.py --checkpoint_dir checkpoints/tiiuae/falcon-7b
```

Finetuning the falcon-7b model requires at least 8 GPUs with ~40 GB memory each.

You can speed up training by setting the `devices` variable in the script to utilize more GPUs if available.
Depending on the available GPU memory, you can also tune the `micro_batch_size` parameter to utilize the GPU efficiently.

This script will save checkpoints periodically to the `out_dir` directory. If you are finetuning different models or on your own dataset, you can specify an output directory with your preferred name:

```bash
python finetune/full.py --out_dir out/full/my-model-finetuned
```

If your GPU does not support `bfloat16`, you can pass the `--precision 32-true` argument.
For instance, to fine-tune on MPS (the GPU on modern Macs), you can run

```bash
python finetune/full.py --out_dir out/full/my-model-finetuned --precision 32-true
```

Note that `mps` as the accelerator will be picked up automatically by Fabric when running on a modern Mac.

## Test the model

You can test the finetuned model with your own instructions by running:

```bash
python generate/full.py \
    --prompt "Recommend a movie to watch on the weekend." \
    --checkpoint_dir checkpoints/stabilityai/stablelm-base-alpha-3b
```

Output:

```text
A good movie to watch on the weekend would be The Lion King, since it's a classic family film that everyone can enjoy...
```

If your GPU supports `bfloat16`, the script will automatically use it.

## Tune on your dataset

With only a few modifications, you can prepare and train on your own instruction dataset.

1. Create a json file in which each row holds one instruction-response pair.
   A row has an entry for 'instruction', 'input', and 'output', where 'input' is optional an can be
   the empty string if the instruction doesn't require a context. Below is an example json file:

    ```text
    [
        {
            "instruction": "Arrange the given numbers in ascending order.",
            "input": "2, 4, 0, 8, 3",
            "output": "0, 2, 3, 4, 8"
        },
        ...
    ]
    ```

2. Make a copy of `scripts/prepare_alpaca.py` and name it what you want:

    ```bash
    cp scripts/prepare_alpaca.py scripts/prepare_mydata.py
    ```

3. Modify `scripts/prepare_mydata.py` to read the json data file.
4. Run the script to generate the preprocessed, tokenized train-val split:

    ```bash
    python scripts/prepare_mydata.py --destination_path data/mydata/
    ```

5. Run `finetune/full.py` by passing in the location of your data (and optionally other parameters):

    ```bash
    python finetune/full.py \
        --data_dir data/mydata/ \
        --checkpoint_dir checkpoints/tiiuae/falcon-7b \
        --out_dir data/mydata-finetuned
    ```
        !	      U      /7      I      !\      c      ƭ            f      t      p                 %     V-     q3     7     ;     @     |Y     ~a     i     r     {     [           =1Ǽv;+ɺ<x?%WpB=7"gg;e=o;fҠݒJ&ER¼s<&c^Zt<<*<oF*<mힼF<i^W?__=Ek)ϼF4QtdY=gi<|Aӕ8;đ6}w=_Ѐ"<h$<OA=O=Q;pMS<ּ
<FI%=x80Sƕ,i0<c9 A=_u<N:<J<i!^<j0	=<zd>m<}6s+SU/Z=Smy{;G=1)<C'=I[M!;_J5<vGX<;2=h`]&=G=n
25>&<zvU:<7G=7й@mi9; F$?PƼ#<0ȀT!xI8<
b~S,\>=<Ѽ
=M~@B{(<x=[;}VƜS<<͜|J< <~h=+Xg@i9ئۻ@лkW!=/=gJW=ܺ%<V={|'?"<ۼμH=]=&;
==D{,uV<5Q<С;m8+<nC<L<	=upi<a<Io"O<{ԜPD$= G'7ٓssM2[6EfXE=\^=5=<<0E<_
Y/<rm`i󼅧-I:B>;42;K
~ջrLK;_ <}+;N<:>j<Ì=<b=%$%=gf<auduG}4=F3+C=9׽"=~i;b={=D;UeX<<za<HQ;=OCX0z:2ܼ*<E?=3J'p=Χ=(<O<es('<9;=f=Y>*#=@]MsE=(={<<bB,<av[4=qۭ׾=hN=3]L3<.#^⻻;F;,$P*·=l<]<μ)<=}!,T?<2<ԋ<^JNT=Zs{BVp.1ͼ )<̗<=0m=bGzٻ$׻Vr!ذ;;9C<~Ȳ<JƊ;T1=qAk+<z<$=v0==<Ⱦ-=i&U*>=}P=jZ=d9=r?P`@\7;H<P[=Cj< 9\X<:@>i$8֯<P=P鷼іFL1<<E=S<<:E͈<9.L<.Q̶HI8Jݫ<}Z=eM1<x=CuB<: =cD;VYż=?;<QMÃ<W]{d=u4nC<ocy<,kt<̺sϠ=Y\=b\mtGn=<=WP΅S/lH=SZh_׽\k|<0<G-<s<ad<Q=Cp;;z%;K];<*<eDFJ< I		==,K<K<w~<O𼔝wH5	fc<2<[|t<3W=P=[	ū^;K;:6<Z;ץsY[.=_!=<є21<FU<,<mI<;aJ=)vM:ϼR<:E.=`,jS%/;l(=09CV@Q==v^j9=0Pɰ[=;%"="~<WS=G =Sk~~<;|=uYh=?)<VB=$M<; >e=^g@= ~<<(=C;c:^A=Q<?5;9C;D	#Ba̼5x0F<5H\=1<X=2w^ߤ<3q=A;I=<YIm=u=tOHwC=kC='G<X;{:FZ	&=-Z<»"\*;Gz2`(ˌ<?2=1 =ƖJ<1<Ff=ޛ<?:6ofD; <=ꜽe#,#=<G[ZGɼۼ<bE/"=}$=gs=*j(B`
=ܼ<wK5>u<>ty;VBAr
(<jNN=	ҼѼ#8n==o<5=C=o<<|
<=<^q<~P=Ow<a&<u
c=4"ļQ$;͗=~<IW;=h5=XArcY=̹2<-T<㣻<Ug<`<<UZd+~-28 {؞;dow8z)6_6<1}j;%Қﻤf| L=s/<z\'^!=<Ч׼u/[~=20I<<r=L*2II=ĐC<y;<=;{.U<nO=6=,=f蘼<;/O<.Fy9nn}<iz<f%߼6m#<o'=L̼ =ؼ᤾<=z<2c=<3=(uDf=Sn)?;8r,Cun=Iϼȳ18=+Ƽ<Ma)/=(xT!-|:D;¦T^a1?/<Iʩ<èW<;_hӼK=:(<a=~ă<Sһ!yGoi;=
C<?غ|<9<<<;+=^m;9b=	&JO<&1<ǼC\Q=9=(<
<lw}w/#=9s#<Jp<|;l;	U=Lq<>ޭ<4;=vv	NB;Bv<4</=h/l=#
=A<G=<<̽#=zS;c=&h5P<@Y=[<D<=GK{6_ 	FB;wn=su=Q;]Vg<9u<&6<=μ%:E9nyum:O1[=<:;r=D?RR:\=ڥ<q;2u̼e==n=.<:p<_=b<=^<zJμX.d<I<k=F̻@:a!e`@=}B80;NP<G<=X;zp=;5<6J[=D:f <黻<lN;|,*3&4>=iN=>뚼@[SK=,(ʠ3=$;F4=m^͋<C+<0=w5=I{<_a<ļ=}t2 <ۼRZF@;. -3ͺFKΞ<U@Bɾ<Z<8F<j9<h<0~<=~l[eb1<]s=c=Jk`|=z3<ўX;».n$?<OX=PYXh<FO qͼ=>/@<H3Ƨ{_s<^w<=e:<ϴ;1<sC<[>Ѽ{P=7<'$&=9<k[<2O2<];=cT=<kI<"	_(=}1;?=X\<q6<*$1F7GK0<br񇰼5!>'=,
~Ro-<C<RS)=U<;4"C<c_<b<U~<|
{5|U;t;<׈yc%ٺܟ<Y<!<O8=P1>:{f<s=D.Լ`q/=eJAv%q<G=~=k8<2¼<8g"=<0<L;QZo^=3m<}!N-< =)r<@<g<q<	s#<:Fl<[i3U=	<bO"=w:=S$=Mӱ:w]Pٻ8c[=`ym<琻@\<=ך(p偼l<_G< 8UFE0J<ㄼNS2E"==0­щ<꒍=Lgѥ<,ui,8J78=Ҙ:~ŵ<fQ=SRhߠ<g<L`IλN= =u"Wړ=ͼw=\=@e՞<=<ӭ;<J=(0=B6k&:@y0=ñIg<9U/3Uњ{<o<U@?<*'<=ʡp==j;;=޵S<jiռ@qI<8 ;G><Xpyju=*<	6|;?[7=_\36ܽ<<;Lpxn=V=<!HBJX=1;49Ӭ=G=ٺļos^E浼 P[mh=r<{φ1=>;0$Q<rZ:QO<=JG4=vu=<<< =B<Q;b=ʚ=z`<ʟ=,=ȳ=#%;>4<Ys|ػ<m3=35(*<
v=B=/=?E;,=af.a==<ZȻA8<N=rV[l
=:Ww=ɐ:Lf01T<ļYH8g<&UV-;=r<#=藉!a;Jch=`n=D6<xl=vt;;߼w<T2ܼE`^(;R;0)<=+yHXҼ{
=L¼Ѓ<¶<'(=	#k%q==$Foa<u֔<V<;;<99` D[<=5=*!<Ĝ9H<5;n<#<O%:g=4?<<=.I+;\;=,.<B	UЮP
E;cH==~|'5G<&= 1c;6߼06S-+B kc4=˻终<Y./h p1oL=xq'_`"<2 }<ӰF<+4art:YLN!<^=f<{;}w=Ȼl<慼k_z<2Z<}	 =fļc^<c}f<Ϧ=Mi=(CWe:쿐<<q<,<c4ʼU'<;<;=GrQeT<mcbڂ<Ea/=o<=?<<`C=bt<<Hxl;9kj$<cwySN<#=pe<0N<<bSżs䛼[<+o~<`ak<Lɂ<:üf-:'<=I<;;d7sS7F<~7ͼضT}`Hb2,tHD==.<!;<6kɻԈ=J(=M<<Q9;˻fH<7=89%ka<@<<={:X"@μcM=ca;^.t<^I5<%<4NL,Da;<)s;/6$;Qc\$lX:ؚ1&'<m<Ž3=+`<.<yj<]==R֮j7;Rn=#h=L<;<¬50<Ρ;uO=!XU<	=.<-<>V鵇g&EXfgk/?q=zR==s=#,p%V/l:-< I;^Vyk<[BU<p<4-=@k=<pz=*7;pc=4<}( ;{˼"<;=b;=|9CjG<:u}d-<J2=ݮ>E;= <'b=<5C~{ɼ:A=Rּ<@D=9k=j2i<Ք{=üy3=ꜽL^=ἰ/`q)=Rݩ<<R=<Zpa
ʅ<yI<#t(=e6T^x:ۤ<{ZZ;V =C(<m)=\ <@/(榶<*|l8^=A]56Q<kT(L<E<q<\J@B׾:љ8;9<?]=RJl=1<=[5=Qyg;5=<10C=7-P)=ip<nM'=}{P=*<<}<ɼ) s<	:T]/;։C<D(<\φ=<醼i<;(=)^l@UY<^s;aJ˼4xw<MμJ<y<H=E<M,=:jB=3A==Ꜻ}=4<-U:=4<<h%y=ސ<)bF_"!ff=*<&A<< <7=j=à;Y4<ׯ<tS= _n켷 #=IRջd<t T<5<><IF<#ϟ [:5=ZQ&=!2/=VgJ<m<=-=@:enl/Ux<4.=h=A<&=q}=dO~6<޸Pq98⼑=\I<.=i<Qwbb*6~'<ȫ=/Xq<Q/[S
LV"ݼ)	p&3qN=oļ/xCFB^:N81=V[=<{
<1-,U;)E[!=}+/.иOs[֊`[:<%"I'q=j39;;QR'ռI<"JD=$+W<9{*6=m<<<so=)<*Z<]<X񲻦p,Q;P=k"1d;Qb[;C==_B<<q;4z<`&<WJl&˼<N؅̀x5 tOr^3R<G=,}۸Ԍ<<=
D0I<m 
<R<(<T]?=ר	C==	0eռ=-ͼ(iḳI@<=N*ų<*ʸ{f;C<7<P
;Zk;`b}z8Ǽ`s<bC3M;M`<w<=j޼ :q3<BR.=7:|O;$r=<K<Ӽ`<R?<U=1S2=Ԏ8;]=L 8h?=}"=3!=|j=[=:=n==ωd<L}S0NTl=ߵ;/j>
7c=  ?;=O=s켧;Et:״r<hV#=	=#X2=|=<$<I\==e;<k(s<<+<R<<:vȘۺ ;=Zjh;];*3
<EB<>@K;<AgY?==-w7=[irK;(7e<Sh%=eu9o:G2=#^<<1g<B;1%fB.n
	NŦ0;\c(Kv"c;;;=GQX+s=a<K&/=;_Jˎ=W<P<h=f&=z,;	!+Ϣ<5H-?=#T<O=<7~Q=&c=	<x:[;9b8V!=h<D\;{Y<.hk*^Ӌ<---	aX;7mYE<<Vx<T&Ǟ}IS=">	ڼѻӼ`촼OX=Z@=j+Iuvs;l<t <eф;8;5<eټͯ<?!m=;_<U;=Y<O6b;}Ua8<V!C=^Ip<T;=;=:ORSᬼO1cf<߼Q"@޼:)LZvnQ  A"=FGw\= F<(^v<|J=.t.==|;À]=N2x<á`=M<u^(<\Q?=~d)ǎgA0<T=ӫ=ҕnpv0ʾ.<d#;HM\=S]YؼêD<k?ɻ_;<[=4<eG=ns.>S;ON）m8=<v}gIl=})s	(;l8=4XP;+=s	=	r*/2=\S<vq<#<VX<!=^<Ю_*9%͔}P;W<j\=(V%f$ T=|<@<:м4=		g=Qt|=ڒ<$f <OA=nջ=$<AB#5o=Sp,ZS<`ϝ=GD8=;O<0X<c<a=td<?Z<di<W͸\6E<{!꼓}ݼXC;ϻNQu.=i)	83n<
û2ZHPQQK;!e<5=)Ѽg<[<!mfn=ۯf?`<=D%=u<;k<	;<hgEKGz=3+!=bC42=+<;8<Y=5=I]<1=bo<oռf$;7<2Uؼ}꼘f=TF&=<
,.<B<FK</ =O\<
2<j_&	<_wJ=^=^:Hݺ<61;@6Ђ2ռ҅;ٹj=Ҽ%Ou<=1	=ZW:)[u=`M<'=7:)><Is=']<nbL=cM=V;yMɺV;|
=GƼ侼Qk|$=w<9F=O=.rS Q;8:"s</<~;y˝+9=<<q	=<=O 'o	E<yX<Kdѻ]=<+S<ƨ<b5<Za,=%I=M3 м 	=^<O:(Wt<Xt=e =Ru
=<lݼ<
Om=0 O˻8ؓ=m@; o;1=?<O=<w"=vi"Go=|@'=<n<ɴ=I<v;</=2Ӂ<Xu=r:y<R	?=<;t<bb<j^GFl<=ֳ{=$Ob(=&&+=ͳ=-w<=&)=ɊּwѬh<8ˉ-<)ºQ=Eǘ1HF<:fw;9:<e4,57=ZǼȎ =V<5լ˵o3JڨkgE<e](1=m!=ϺW%Px<"?-F<==}<p#*<F̜v*0==Xg<j»
<ݒ<ȻP1=o߻
pH=8~/ux<X;fD%ܼa`<ś;<>q:oK6=k#K=ƃ<u7<<}oIk<t; h+A[ O'Cc<IS<;廿-=h<˼!;=лǤNx*+O<o<ÞK:P?%(X	;=FlP3=3`p"I=p:G;pF1k<P<Gt<O<<w=p9=	f[=.u=h6hY=P/ǼȲܼw<A=۠3ݼ")<4;Bwxo7ռ5=O?\;P;A#Aqp=ff=׼o;-<?˧<ۼt:Ἥ;fK=Ķb<<G=[j<4;
&ۼx:=+<J1=;;|l=.(<Zx;Xb='<sm=P=kK缴(<io;#<jó;=*><z=;߽MB<`J Hz=U<=a<s=!ɫ<No	\<<	O<ь=Ƌ;iHf=n3so!=q:YjX<(}E
ܼH<X	=[/<kR¼?!L6aHV"=1=]]ajWMcQ<] =صtm;R<=AZכֿ~%b<)%=R0D\<	δ!iaM<5DX=\EA=g<};a`<]&R;[ݼKٜ<"Gn3<c=Qb=T=xgX`^T}<߼Z<Z<&F=:H=[
	E=+<k=_`< =8m{=?u<F=#=lʋ==<~:(|<@$Y<˅;(	;ʢм;k<t;d|<؄l4I<OǼ@jscD=j,i>];	EY 34{E=l◼ŞخY?c8=,Ίtw=L=i<nl<.;EK'=;D-L_.V=9=TXº3W=n ;<`so=_<<NSW3;=Ƽ[><+V`nЎ<eͼ筢=ak;n
r_T=X;L<P;W
+;=ټhwV<#;f1:v܀8=^QEe\-
=a_HQ=ݼe<2Ib7<̽.a:ս_<{`<~'7V =<<B<R<rw>>=T\<A;P=]a{YlR<4,5=;<9DDT<I?ifo<=:X;R<";z=m,<;.m<rr;1<'<<	Ї=H8:$u=eS&=_m6<_<<jc<?=73=I9<;
?ߢNE!<˼=B¼^xӼCِ<N(
=s{Süz~|=I?E%='!7:=sP%.(ͽl*=mpѻOhm=ϩ9R: <eJт<ij='L~@&=#׌8<=;n4;n<687.<iEƻ^;ټ+owc =h&P#<	K_#<r=9=Q=Sػ˔Y犽Cr6<備<< J?<sE<wϼ;}=ۿ
-<<ݥ<t<oC}ZkP=?=-+F<!È'<Cm=E<mzn<>84<MW=2;<ڵ=<#Q<
T=%=+ݛ(D=fE=]tr먼=A䳽=V<żYx;@ 	=M;=;r!>Q<<;=(F<MB=Ÿ,=[1S<dټ
={'9A<Xnf%[:>I`=K8'7rؼ\̺)N^:<
=C =ϭ;ǲ<J;=WxU=v?=OO<Dȓ-n<A=<1e6rG=	=oVTW;<|=ؼ=Siu<<Y%@*1-9μS<ɸ<b@lH=i<.e;b^2#= 8jB=F%ȹFV2=O<=,X<<;s=O	I6=<=^&<V<hS=:<k;{DgF*=39f<g<	<seӼ,Uc,Ǉ<BtF=ȼ{!<;p%=J=`=(<٪P$tmj=q=(k&v; O<ƺ_sR9;φǮ<;E;1Ql=Ve0=L=A<^<	@=#YK=q<,==J=H+:p=<7IUc9=O8D[<2*<:5S<<qһD<}v޽Z i;&tR%<\Y=}z~< 朽KL4׌=G26G[輼W;E£+ុozU9ߢ<Ϙ<B=1
rR<lwv7|FW/P=ӭYP~DmAh= *=V<Z;xa z_=_1Nu8G?c?<e}<Pמ=@Zi1=A<S]<4=/=><w{
;+?h:n<*= M`kXw<.(nF<Df= 9D!Yr;Z_(=+:3B05a)A=pﻱ2-=cU=;r<<;.g<BcӼRdMͼh=?C<cռ]=	/ȱ=@<ü1<oՍ~fX<a2=#<Emk=3<X:(ɕº3<vw:."O<ϭ=<[-<w:<i #

0_=;H<C|Z0/Z%X_f[
a=:օ/;=`,=q<mm==c+=<$n==u<(;?-<R=t&W=4=3	ݭx|;5Aic}PB<><a	=;<yEg̗QEmDB:F3Ž,=Ǥ=y<H==L@E%c漈]djh=L0=<ƻRj= ;S
!䒼*x=&=t<y;.șeC#ߑ߼?'=hP;f׼N:d:QH=C;=P<3oUv=eP<żti<=<YԄ'<R]wI#=-1=2m_;`U=O!L=غV8= <߻A?;R>U+=+NL0󃼤w; =q]a3=G8ȼc=xYs<&Y<0:<r=;G<ьV6=vT<k=<lS<8,bg=,<#<rҼs=W퀽=TN4<V<ϋ;;#\==iJtny2@_<Jl^W B	)PኽmJ<0Cb;E$<}I<>4=ek?{9C=0<M!iCATb<찻uB,9e=Ǽf=dWT˼T𳼼{<뫼t<@ c9.<#q	7f=j=X/;E?=H =8Z<</k<])< A<w =!Uu,^׼Apn==7B3=vIb]9;G<0<x{:n=C"ܞL=JBnDiXZg}=R7̼$b*"ȣ=d<sBA`=:ܻ؁5bN5 =$)<2W9	J<V{ٻq߀<MJӂ<_Lû[Wk}={Wv>[Im@=e;O<&pΌiEZK=c{^<܊]<2G9?uǼe$<J<k=6;;)i Βk=1hڼXj={a)(=q̼=N=>߼6=AaI<)b>SP@=ʼV줼~=bO=}i<`{<Q!<wm;<<p*.<F(=<L4/滋j.=/〽<-X=)M;A}=	;5bÀA=As=ne.=Wς<:/<?=H6</=ˢýj<J a=4A=g)8W<+ =ram=<2 -
@m<	SA;m:¼; <Eui;pb=j_*}ؠXUVƻ=Z/<Pc%кe*=9$=ѓ;5ג`O<3v=/@=ϼ0oB_W<#[=;=M9:7;=ci}o*F	<bq3W<=I`;d=;S=V~<y$<!{&=F;.mVq	c,B>B=Mg34<	ͻAA2Q	%>.m='_<j`,.W6=S;wg<f<Uw=b=Ku= {K<Yu6㱾ϼ= dP,=icYw8<z~M:<|.W@w<FW\;tLG4a+=-_;<r=Rȼ+뭽mڼ;\z<2գ=Qjv"^,={<7~<]L:X.j<\;=
nYd<K;{<-{;?<<!Ĩ<Z=)#mw% ;;z	&:= 2<7pDW<'<Q
=RvO,<4J<_"=X8<ch"<XJ<6r%<$=@J=Ѵ:zIjMrr:(=8v@= l#sxo-=-:J=R<
=W;UggSx=vcc/x=?`Hм!K=j-;;6c=,G$}VIb=^:oɻ#;	;AaN<Q]E4Ls=q<\Y=y(=ֻi=e?==b;?\K<a,<Ļ˼A;!>K/=A =oh<a]*O)<R&<9==x\g;7tF =¶WLB< ZcCcB<' 8!<ofF=Z%]<n673l:<|ӼE;*=c=<ԼV<<pn<I
=
=VE k<*g=;;tY٢=0A:4R`</h</;k腽 < c=#=ꓽAM<=B<>3ryĺu<	<<D>ՀC=[F=<C<v=T;&;!F<<W
<!:7klk%<tP<z'ݶ5<<2<a=A<%(&=o;HJ.51<<;N
<Ƽ,,j7<t<op	}=;Ƙ>;HI&I=XkpY1Vhz"=M8;S'<|ʼG!<Os-/G<M<7<==cu<<	FQ\= b
H/<X=<X	%=CѼ@A=#H8/=K$=a<ݚ<<E<;y;< [=T<lE*<=Y=/.=<RZ@<?-7(;=(=<7\V%@\Ox=K=ۗ=	<=t=F=9~|3{=u1<;S=<Mu4c=T;.=&:
e;.ջ.<Dha=MzbSn<oH<")Iצ<o9'=X6lo';=r{=x x*$<<+=q:ڠ,"
5;"ʤ1A[@=p@K<#<׼Vb=<n<=2=%	l`׼g)=<>>;P༡+n<)p:<N1.&b=<E]<Yq;;Q<=P<q<JuGߌ=cp<ۿ<<<9{=4<ȼB8 2_j'1<`=<"erBX1Bf:mA;1i <;`l:2\U4I=#=J͔<<!|=1m:*+y1t_0\=+=<
Rb<]2=]<3=E=C;n\<*_<|P<<g<Ec=5
)l#I-<<i8=JN:0 8x<Sڼ3* =G]}?S	=؇B%(5䝻ۭѹ6=Iɗ<SF=P-=<M<7zV<3&h$X0=cl-<duiY	5<<W;|<>aa=WuX=	V_[TyXI½F6	:!a+[J¼<<ӆ9[<%<5o{VJ<VV<uux=LCpuL=m=oj%><B=zSW?<[H<߼U<^I<>=<H<==?[<k/:+cj; ;8
$-=}=<=ږ3@%<c=Y=)=_<$~=< Q}U<Sϔn=:xQݼѝ>z)=^za=ZD<;9=]<;+;Iv%<n=j

=:>=<뮔D=5]\<!9=(n=:%=/=
<LC=&IE<Qט,=/bJ:c<Mϼ[V UIX><d!<<ֳ;ɺ^iF=|zf<")|o=<^f=w
M8 &<ᅎ1U==B=@s<C<p1!!O=(Z;<p<Q=<6<SrW<b澼&=w˼#bP<5Z<]=?-V-<<;W;xƼo&Ikj`=<4dbR<y</=Ǽ>S<Lfm=<M_J+=<+I<YĆa1</YV=Oo)S;4<a<{{2=<w<38j'<X<
s<x-=N<>o=s;Cl<R=^kb=!DN;Y=a*P><=a؏m$<rk@=<3쀼
;cx-t<X"!X/<6UJ<ʼë<YtHYUp:y;x<(
=ݼ;QXݓ<J8;<߬_</<)'/Uj7=/Ȧ;1&=v6`m>=YKMvCL<6F=O<=%
F<7PK)=0<bϽv=X=6:AI<<i( )k<-=Ǽ;UK4=A<.<֞;+s<漍A7;#<	ԞgE<vW q7=!=>";M(id=<H~dLtH=k<Vo=R$=-T=hN]2ڻ>T/<Ħ=&.<!^ =̛N<.;RO<Lk -y*V49=FJ=q;-V<=ML==$&6Iv<;==x<ؙ=G5-7̼hlit;h<=/T<ګ=N7b><wiiB7o<a<tF܆}v=ʳ<><񩼿O9Z3ܓ=K;S=]̼$c<r$=u`|<"z=m=B铽ཞ<K):<c{>l=ߺd-4;E}=8j!"<%M;짻X;)XR
^=qyW*;I=@<_|==lm^hY)8=dS;<'<isIFcf<-Ƽ;FE!<=h{E=URdмS=<T<k;:X̼Xݻ<sv*Ϻ(;fv<===;|谼Q@G<.a<-<.=hی^7' <	===A\C =թ;~TiB;<2Hg伣2Ʀ <cWP{~ȧ{ͼ<ya=Q</{J8i<#?L\t:=L<YKҬ9^cBżR:;*.{<ij<<gp<WN<(<ц;<d<2!J+p;x=!Kf<=cDCM;"ּ?kZ`Bm<<~;μtK=Ζ={<<)=΀o='M=1L+=<\:={-?<7=ܚ;UɏP;>79=}=]Ml<]qk;X<-@=0G<<;=Vo;S<rM=#=!ӽ<K}9Pe=Y/qO===W}AUмd0^hͼp^?<ح<"";&$<*ȧ=W): =n0Ut%=<<|< *<=:=\<o<$`<$<d;X'=WѼX=7ᔺ<<@V<"d;߯n뼶=G̀	<e	=4Ԛ#=Q';$49+<ހ`=&ji:>dtۼ9[r<ay8OW||rB2;r<.s9Fj<Dg<tX-&<	=J,@|/M^~<4(<n|<E<F<a8yx<3E;p<mY
N|<5o:e=Ӕ<6y{<r\a<WI<߼1<Y+(<ywѼ<L;g9<|<F0I-2u漨<P{q9$O0><dY=>Jq=Le<.o̻)7#orU=|Os|<Y<K1<C;I<𛫼+cl\C!/\=~=5`o*<=Bը1:<'ub;@ A[V<H.R;iF;?@ 
#qܻ&=ݝ<ɼa]!68T=?i5<%+=%ĥ<k=jt<Eh%F6d=$=2[<A6kf<Ȏ3<eü Ȗ<'<[<Ey=Pm[G=A9{[ª}=sN<"< <T=f&LV1=rd= =2n%<b=nC<+	<lǼz9vfNE:;<UX+y^F<嵰<EQ!;AR<,<=8;=&j<]ST<er<_䯊<.һXJ<*;ܢ<V<Ěp<_x|i=c=8Zr:<~<	2gJ'=U9;p*-<u+Mn;Z%KDTL)<fO;M9+<ˁ=N@`ya =J;*{<<X.<<]'Ǽ=Zz<9Pql޽}Zև#=m=ĒCË;7=R,7.=V+EaLR=^;(<0)=3<¸2=^sL=<;1=׻/u= TiU:=<0ѼL<cNм*=K	X&<][Dwd<^<(^=#bl;䧼O<|]%Q2=cՅR=*|<W=G=#=Ƅ<bJ<x<XbI~XI6μ=eL=;2<Z&[)9w>7+#<!ۘ95<ټn;=F?;әz=TcU2={=|徻@ѓ[Hs(<=>ۆRM=żN=<ӆjaꅼf뻖<<jx
/<hET,;榺'p+=;N1#=i=/d$<Bq"s</P2=<|^B=\<-܌Fe=&<<TY"=
`,T<0;-\;\
	+Z=&=<&7<FL<L^2
͓FU=>=d>)]=ш;X<i<rȬ:A.<QUBiyu<i6;5;Cr?<9`/@73|;=<0Cq^J=Ru'=Jp,=;f<ּ32j<_ <J;o q=^@<Լ\=<AB=)==L=)"[=є;ݺ;h7K=9;=)=k=C;.<6<H=Pd.<T̼*=_f޼B<{B=üa|(=˕QB;@"üT<73˽W<$u:4w<y׼B5<.Е<0<P_|J<%u$=XB<T:
4hf<谘u<Rc#I/;xμGwd<7x=?p	n=H?=!<IPuq7=9	5=r<Ti!_=_̼/}b=Ѽt~Ǹ;;P=jZ<w<:.E=Y<3Et5I#<=	㵼0&tC׼mw'KӻԊN<VI=Pp<uce<)<E=k<{=g;H;7!,WK<b<<>	1Mɘ=X*AX;-jTG'Pc=<<KBfz<~K	<*::巼<
)<͕邻a<5<]&%;qS=r@:;LGyn=Ï9:N=<^.=Oʼ+3=U;?|<ֻ<A=R==L}<iƤ<맳<='<.<R;l$>HN7<e=Q=ӂ<Ax<ڡ;d#XbO==B~2U໨24FKȭ=y<=';T/=.ꑅ>}Xl<Rd^hj=ڌB=ԵF7o<=9=n:BT:(9b<;CK<T_uM73&:Q_+.uI?=<?8<='$<TԂ<{/=Ιvܼ3=u>_;0T=BaS<@!;1&H;ռ/9x4C<{|=O ;Z[WM<=<I}<N<J=m+0f$]xǼjz<X=Ex<44u
<M;==K<`ѼP<& <,U.2=Rs&a<UVM<><~TMQ=s<j=8,=A<~ <Nϻ,0=_j;}[=#;=+X=#6Y<=-<Lǿ;ߞ8<W1'  6<GeG^< IXo3{H9z =pV<m2=2<<K]S&;u^h&qY;2p.N={_<Y]z<<<f="OAdF=}Q3;u{63{D-;,G<އ='<i	=0=Zx</<Z땼$$/=-<]	p><3;2<^[b=P<>;I=]uQT<þ!ݹBCE>`:qZ=LL#
м=;O<8HD<mv=I<Y2/<ZռD<`5A,˽mom^<[=_<V-[p=9=ġ<L<9<iz8f8=%<hK<-v{;S<P"{%38<>Xk=T&0/=%>%⽩<9;]/<MV;OW?&=5<Fs'!<`>mkE`oR=	1<w;p毽<ؼ/<t.<4;5=<#29 <;<} ;rF=7]ڼ+z0=f@<p<p <mi9f=Wυ=%;8<@Rϻ((<;:żǜ;PĽ"<h,qm=]v=:k=;!H?<r=LHfm,=H.)Ԗ;MVUiBTFg=^&;#<&=q6EbBhisӻs;~!x=ha<3</(2<U[<=&h,<1=.O乡==Y<T=aܼїd܂;=|!ɺ<^-oN1:ۼ<-<=EI<Q}=T<><<D;-<d<i{0<a"#O<VevF=AмX;=b6V)#Yi@K1=;dn6I==I$<6l<(`+/"L<^--3*Ƽ<==ʼО8"Le߰9k;NiC?s<c:F:'臽dV="9!<e=ӁOO<^;=щ=q:!=T$Ǽ?8<;;n4ＸG'ɻ~ ABN$="ȼ~44WG<;Rۂ<ک="E;e$<<GGB<';~t@=;ڍD<4x<f<6$~<\K;G<k><< <!!<[μk}=4[=C<᤼*H;?=~5ɼ<=C$s=#<);9<f <tg??XG<z>4e=ku4׼q 
;G4y/i<͌</<U<|^/4
Վ<
=7$:DoH q<j߼  gl<@P?=*. =<2ƻe=<< j<k~%=Bxm<(<<=Dl=";Le[0C+=LI<?K=ɭ,<*;ϻtS;[9=Q]p[$<(Nǽ:BҼY;&<@<C<d]=<mEN&Pn9>ü;Ѯ6x=f=:fu<2==PIK)C=<$ʻù(<Ј+;=N$;M=^;*jY<I<.e5	׼%%B=uR=E:|e=<q;pGG:P<~Y9$D5be=W==eH=m =-7:C*<;r]=&J0[Ľ1LPU:QƼw<-=cżv=ɻSrgd=q<?wB	/<<p<9B<C;$<Pa	zSb<;rY»u<D̪\=b;+X,=;L|<Kv*ѻ<8'7뢴D=30;{=<t(ԼF=N{1?``;9=퐺'=8qZz4={7K]=º<; =6T<9ܻ<m<U=<`p=FJ=(<=k`;ŷc= =[A<+<=;cԼ+Li<;+<B=:<©1稨Լ{
=fe=@'=C<<z=<}ϝ;<3;S=zb=jL½s;Lz;2m<q=F:Qȼ0=[=z»ڀ2v<R<<
;h ؉=f<<
;d(:3+h=o&=ӶǼY.G=cz=4;O^ ϼF͊gr۹M)8^<IĖ<,-%=	ȼ=/ թ<9SzqF̻G==,<M<08Z(|;ݗ<F㨼&fӯ<(){ʼM=~İ<"6 ;}<'<͐<5==ɟ<vj9<<=xH;b	=#i;9?<-=Y*=DiJyHo<7=<W<];<oc;:⼑BżPL<Pv=:29d[	v= k/3=T<<I<<2cӼ4M<J:kC;~ڳ=<8;ۦ;bR=<M<TdZ<<%~ݻaq|=ճǃ2Lub`<G-=Ӡ^<H<;H;r<^hb3<@=$F<m]~䲒w5=˟!V=z<y
=F{=<K=Fc<XP3Gb=Ć<菑өǼn^|;ȏۼ	1=gg%qG'J= ,{-P"<Խ35/e~<_6A࡬ʹ~Vx<//G=<13' <?N(;{żB=:ٓ-jw=|{+;DT;~+d!R<=^E2<R7cx0;Vb;&h<Py7<Wu<64=h40=yy	L<bPɻrq=7;ZK<
<h<^ҼT0HLx%=rd"~3=k_<<qLq<So0G=` =K;2W)A<<W}kw=7*==r<=a<2'=z7w<<:}^=ئ;
<>=u<߅ʼ);d3<(@=EԻ4 o;f?i,M=n=/;
!a;k<-$A 4=1 3)<턽VO<=һ.0=SVX"o㼏Ƭ؅]=‣;;t׻:[6=7=q3<0mް<=*}=*ɻ፶U	H;E.;D=x'VU3=;c[%<DW<e<K<CԻ3Lz=<s<@o:v1]Ma<z;?1ໍJ꼗9=y7)T=XJYcÐ7rIi=Y:-ټm<-T=3=)<$H<.=:=w<
<B<a/:\<'`y#kV =(Rg];5:ETM"	Vd'<jk2q?E:&=l<c3Jv;ˑ=X<oj<1=՜,=5YM<f)W5W=Q=W_Go*}=Q;"cou	cBZzE^<@=#r<X4;Cü;=Z
<*?R<+<N᫻1+<RE<S=ɀ<S÷<90#<<y1	wE=*<;yfE=Ħ<_TV;k5s<co[=Ӧy
	뛼H<C@	<󎅽'x<w0=S<<Ԝ:PǾȐQt#=Hg_;A=w<$2=U`<Kod& <jf/=v<ۼ::B;ߠ<Pr:mɚ;;'N4(;DC;Y=xVٻ4<2<%=o6/IРw==RV2FHj輇:<	N==<$B%=}=),:C<"R<iIs=ʸh<wϝz<l	=P<];v<oZ<e/ = ;2<vf
<7fp;<?{<e-C=/%-KP<}˽h<à6L7:='<Q<:<,5^KVK<qk+-[ @t==;"</+=eϼ^i`<d$<3<B7n<,<X;P=:9=<<3<pM}=)=^&{Q\=M;Ǽ"?<xӈ<Q{*<2=C.<'<P<=[<	)
<t+=ϯ<Qͼl*D}C?8n<8=)-d=3=I=|=wxщ<۩qu<j)"`#<u =.	 =㬘	qF>s=י;Yex<(<#a==z)()V/=Z%8m+;\a=Ɛ~<GսwѼHϣJ5=S)F;bM	:[=>:Z(ռ<:2rt!'=|y9O<	R;ǵ<ͳ;1:<O=ыT?=k׃<<k=9<$==i=`<m<J<刽b弐,==r
4m{6Q< E<#(N⑔=M=T=̥f=[[B~PM<	=]<~=r0=һB߅G=|=:m9n޿;K#<A}<׸<,=)q?G@=R<!=hJлڊY=7O=>W	=J.8;L=e@<ս<<güvm»}m4\<?]=(6A,=C{=έ;zd=u?R;a9'.==x20h3:F}"<;hlüU<wc<Իl-g=<u=x<<<yU<'qU;g1#m=`<F=T<r<Xv33=gBue c<:t:r==+t<dqI<6R<j< p$)6`};{=(<M
FrLS6!gY=@B;͕;güڊ1=z<:y;{I;S6V<Fr=ά:i4=;H"b;ə# 4&Y,=y5@,c7<:ֱ<E=x=A-@:QOB2=c8=Ͻ<(a߼t˼+"E_ =<H=c\<+*qfrq=M"Wl= ~<B<4=4pvt<4r-<vƼ9u8-E;5NU1n[=\-gk:=;<S##]*,=\ur2Wo<zn<0 D:A.vC;ϼPU=៼TM׼r_T;~)=:{ͻnp\=e%I=![߻==2O$=b$; ܺ<};KH:Zӹw<-v="׻=f<x-$=ٻXݻMn=zS<Wh<3HA
<w%u<5GȾu<t:N$=
:>9==B/H;4<<i=/.<=LWJ< e.=|==@<>2;M2cf=<:=x,<=i[i=&^><
.=w	==mc5<|=O=¸<y2Y)8=V\$˱;ڼ'f:;<ٹ<a=d떼sb8ڼ;<<+8;bT<2=S<n2?<<q<{&=JOp`<@,=? KFѺK:Ħu<|=gHQļ
`Kݻ)`=&<D;<L ,=s<}<{F<#-=<U=<%;m<%:<0=K<1I9/:R9<ɑv=5<jDKs<07au=CjԻILYߧ<h&ڏ}=):Ι:/=z<K:ÛE=A=%=dqY<ΧmFNy;BxF<q:^Q2<,4z^:]j<Z<9"<=?<=z=LR}<H,<(pq=4ѽw΀=oD7r=a&-72ۺ5 ¼y:U
v<c8<7`Ph<ԄRl7xm<%)Oѥ ZV=I	4U89=X3<c\;7<^[Bh</^H;G+r=,P<=<*<NR<K=fV<YY<>̼Zͭչ;~}t<63=Q=.=LoR4[;/;|.<mL^<H;[`:S=S5Q˹5Q< ݁<~(̓WDM=F<Xz<;gԁ<;ߏ=As<#H$}7o;CIU~_&=}->&kT<'[=UFSG=@t<VS1i1!y=_<<q=<V=m=M&='g;9j=P;<	3P-=N;uRl= '=;߯:<<S҆G<R	-<M1,=t>I+g;m
=nٻ+U;mHޏ<<%cü(<=٪;Q
<G<|<**nԼ*3 D켖*<w=3:z}=2{<oCT);7_<dE`<=R<ڝ;6=dt#uɪ;=%3M^0ռ%VO=÷
=_<Susf=mN<~,<f &O0;ԕdo?<]g.=eY=x?(|X=r=P\UZ'D =G፻h;vII*@ʏ1<Y:=|:1<o<=*0;o̼ܖ;SrK<,$x2&NEC<)=<<	8&P=&};,8Z47<qO.QU@2<k<λt=߽<]j_<\}<*Kq-<	`v=G=(yRfm=
;bYn;:= a;Fú<h튻>ѽ<~<C&<3<l;R@=#==X"<<i<=H===:W=p=<;l<iZ8(Oo*<b<W8==˛x<nuм:~g[=P~<ջ`<ŕ=PJt;N~+<<xL<T==I9z)=r
;<:W;*%<R=>;L;==n<.yLI=v.@;Ѝ<9@bt$=z=ꈻü;\<=<x%\=A$f/fS--"o<  ==4U
=B=QC*<#p<f<q*f"'<aT=)O-<9üۊ<<Â=euճ+=j0<~wr7=f:oӝ;1<y>=W]<mݾ=&m;\8:<P<6;Ϊ<HX=Tjy :&<@Q<q<Cs;-="<lz<Y̼7<<aS4t 5_WH<Ѽ>HCy=5C<(;j<=<c+j<`':]7L:.;٢;eM'c= ^s=Y;G'<ٟ,<-JK=@</Q4P;tj)a"<	ʔ=<==A=<}Z;2C;%=
;Gջxf<)K<d<Ag=zkM㺼]<^<D~=}@P:	=<	=%s4<h!='h[=P-Ǽ3!J@	i =. (<<̑=g!=G܇<U"<0=h=SG8}|;e%G-v|X=J^FaPh<[+.&ri\4#.I<\</,=Y|<dֻ ̏)=L8=PU=_G-À<ż\m<t"=n=:b-7F=jT!<)6węmj2=~;<mP={x7+=<&(
Yͼyr<DD:+a<ʻ1S;=%=';($.=-ļ`'=:9="{i=j=HzL=}ֽq=29=d|#Wg=5aM?<_j5׻={1Aw;J>=<]<F<Y;,+#Q<|2ֆ<RT伏]6	='trl{ױg*u$,dcR<ӱ;&7"]=i8:<R=k0qfYq<=5=sVW<P3&<<+={[<}̐Hc-<1<H<V<2D<=.##<LǼjo<K=/.?s=x{y=,=Wƭ;Z<=ek3Jl=HU;P[@ !Z:8D`B<3Kw<DV=3ļ䲻ezi;
=i=藽H<v&1=CS<醓:])iO~źļig;=C<6<UC<{=K	]Wio=>N<Ѥ(ghTY<:Z<֝<!'<|?`&}<<<~8= "<8V="=m*bRn8pĺ:w:$ b=H˻4=gʻرxr=M1s{P<Z<,ȼ7L;ꄼ@hOj|a<pv<HnۼqM<;7I=r</?=;<x<'R?:ûUs<NX;6$=g<PMDH`Pjk<N6=
=H<h}%=^w=iM+=`<1T==+<f%5
=Y:<h޼gJg=?;⼓=3c~<s9v=uDL<ا<Ee<Bj=u2Ӛ>C=
=Lw;+;=yŉu<#x:i~Q:Xf6=_G y7<)}<0<|=s=uC-8:;/=QbVܼW;=P6<+g=Ǹ<{<qg=X=7تEӼGP?1,R<i=0w=Su<];4@<ެv'?=f<l*ӹ=]b9qS<)żF<o=YY!@^;cMW481<)%B*=_<m<YK۹)j;UYS;;ܸd¼N̖A<|š=Ҽ݇	= FuA=?3|D'F<u؄h<<-cX⼻Q\6=7A3U	}޻=D=}=-qG*UC"=~,<PFӘ=Jv<RmrOѼ3L<<J~<+E C)K9;0"+<q<<d<zO2*^׻~=Ɲ<;<mO;J8<p:O m1CG)<qH=,=<`<a	b%)!=#7<sl A<cf?	<$C<4X6e<J;XV'yU!=:궊g'<k@y@$<ؼ19==Ǚ[<}"[<hJ?u)8eIX=	3<ԞT=x;?mC5=	ܼ0:	B=HU<=;o=rx<7J=&{'=01<᳠=v<=_X=~D<bR%)A9ɼ7w<;<uB; ;<ȼ>ռ~Fo]=ܼ=`<_\+=.<|<𻶈5=i<Y=ܰ=~<<i};^=Ab;; gr2=Xt<<<m#׼_$)
¼^yC=<q<ʊջ|͚;h<V =G<'<fk{{=&jh=}a=+!ٛͺc=Ce< =\=ٍ<
6<_82C;,iP!<~F"8<"'u<eՌ=$
=a><D1&<+ytfV{ﭼLV=J	<?;P|Q,=!<߿<9{u=P*#;$_=];֋Y&=!=
9=0û;?q!=̝?l8=Z;<ʼlˍ鼂E<l;ׅ<x\=潩<߼}j$*L67ڼ6::;(l{.<*μ*<_aω=;;92s
95l<DN[Z<!˼=<Ӈ=eSt ;UJoM=ޓ<J<
Wӻ$_D=BZ;5g0=<<R<ӕ<"<͊<U~£<<9:/*=.!N3]\~eufʽ 9铽ȱ0=,[w<Zw$==X<!<$#=  =uh<fE<<_;+=n=Elܻheg<W;>D; '=7un	VAW39=ΖME/=׭,c2{n;7(_V6h<tՠ<ot.='<䥼F@OI"<n<<!;'b;f;d<jwZ=R3@=fD[=Ebчڼte<2mKo}=wgy<,r=2pV`zᐼ	䠼'#[$=t<<"AaV<!{=<7l=W< +='<ǫg%<-==hEq;=QɤN}fxd#cI4?s*B<ݠe=;AA=U)<=S%=g eÞ;AȽ{3<=<^84=C)=Ӈ<keśR<ێ*+z 	=m0kq;< *_$h;<I-*=gg=#0&0<lI"ۼ?_=K=[zzs˼i<!=s=;9G:;<<Io)^<ovm<ϓ<O=PMXӼ0P==7;(	
o=JMP4=e=8=`<=Cf<~=;YKj;!]:#Ɗ9
wf*߼c<	]@Q<YZTa>=> <ٍ;G.iw;dB=xg<oZ";g«;hQ=WP<;}PV<j=</f/YJv<<5;ѮJ	@|tQAv}<-BR:Ͷ}M:zݻ7:w=-^m=*_;].<g]灼oT҅<ߪ<0Q=h;;Q:=.
;FT68QJU<]<<Sx<~<!*=P;+<J[;nCK;nm;>ͼ<=JYrv=פ<Cu`= q<ĳy$3$=T<";TQػ)=<zC=p<=͛|=7)T<M=b<$}~<)I<r"<4=|T䵼H<"ZOHE"r=3=krҺ<OкB<&&=n8<<;ӼW 8S=oּA$=R<.=b8F=i]==;/.<:<#x@;둃<=J<Θ<=sI${3=Fӻ~uh<v|4Zw=ë;}=:u={;>Q; `Qapf<<<GP;V}=Ի7\Ϻ["<i<Ъ6k<nyO=?9G∹ 	=WTo:YW=]|<	=cY<`=l=U»fmsqR	 Aټ `<M=11׼{<;<v%;Ȟ<NgSEYnl<	4jf˻c/#=r&<^v􇻾N9=͜[3y<Ҹ<8=mL=X7a';_c<4<pM0d<zh;==hνٕ/G/8=y<º<B><`^441%<W:Tѵ<="!<FSUF3=;h;b5<G<<L;Ƽ=6:ډ:x\<LX<<D<y<1l=4"	2<}=ؼ	7h<5tٻx1[=MȺ;lͻ<_JY X?|w<`@)`<#z<_ =uV=.'1<!;j<hrBY=zt? HMR<O
=#1;yJ=üzI<<,L= dNN<0= =E;Ę=0<;y=U=Д!>$X<u=T=Y<zP!=EZwt를;<Ν<+ܛh|zEջ@};붼7\<ʑ<;=<7=<V7=n'02l#>=L[}=i;ZҼ쐼=˂:Ҝz=;;q=;XFĺW=q<KLeW<O]<<:ޣ<|!=wBP=Z<X"<qiv=䱝;`=C<:<ue8&&.ɺ@%;*=î=I;ɿ<ɼV<,Qy;Kaɭac#ƇuRc<P<o=rRc<aC~;ό=<8,<Ra<^^Ļw<{3=/R<9bE =<*x^=<yC=<2zxO=T⪻<=@&s=<T=S+<iػM;o>^<9R7EV<0=̳<ՍBYCpzGH;g-Jg<Ǽ#4=2?"Ud:B	=vt;oIj΂;<cG9ZIû*X=&e&=wѡ<Gs;;ZA=<<om;-=~cFEP2)=<2$<L=/g.i<YP;D<-<<`з;ye<Rx._YƵ<_qL ;=|!emiU=.8ePL*=rF;,c=O<i8=-'m$7'=y$4<<.)bP$2=a=new;~(K<;fS/;ǽ"=Pc=L<\-s5ZOq<X"߅#nw<da;͒<=T_=<he׼EQ<=<a!<5;;|:a=OP}k=:6y= ^C<=?ʼD=HӜ-T==3&<<~rh3ټ`P==so=3%V<Q]!5<O伴|v=Ϟ<%ew<pA挽&(`=๿'ܶ<S&=X麧$!ػ}n=V#qs/=kŎ/] &<0K;Wʄ=mL]-<z鱹J4=T<7UK=yF:G+</<^"<<1?4Q2v<F<@}ڼ"SDϼm=э<4d].<<-<k쯼{f;;GPq)=)t2<[Z<HBp}%35f<><UhmpS<C/=!}~{<o=!!=m\<[ά<3..=|<$ 7HѮ-Q=1c<G<K"ژ	=W]U<.-=>n=u-; <UIIæG8a;7t<nB	6=޿<=g=`%=Yb8<ո+<P&+<"<<wYa=B23<@~.KT6<O[쐽r:Y3
<9vֻmʼˬ;Z<.<zW#q:K=<=]x4; lG<H@=$	7c=W<,rt˘<lfsM;<)Ҽ1;ٯ<K;|a&=#eZ-=2r;|jE<W<HA<-=xv<YR"G<"Rɼ_<:n޻ML<4n<z<AY+<Um==;6μ>Ǆ==r!;<F<C^+0=T=H<3	d=!<'|$=Wȇ<,.<s_=]=:R}ἷֻ_<<N=@ qAǼMP@媉q"@=i8e﹃[=/\"i<O6<yp`=j/ P;2=:E=9,Ua=c<~LYw߱<7;Əy9 r<?;I4U =̻Ojn<=
@AF= <<yO@&~uM<<$o<=J==rW.<B"=Ҽe;L<@h%=ގ<;%=M;B+!S;򿴽@9=!:Oӷ=Z\jλC<2ͽ <C=&?.<w<_T.;仌=<h 2;[UK<oV]e-<Di;&Dz<2=F\<,=<<B;ۻhpp@lP<u(vE7޼lд<Ki<PxL<)I=W<Ҁ=l=綁m;|<6<ީT(ǘ:;a,xX-<M8<,˼iBf<=Zy,lBo^S;2ux<>\(<ޅ,<k	=l;y;< a<lƄ<<Ղ<S<aWȻi8Ѽ<]=vE"	׻%=TS4=	ܼ><rpռǯ:_8M`+>=7g;!:)h4o'=廥\ؼ;AE;)=';'=='y<w;ԥ:NXX3|j̗<.2<# 9>qa;G<g6;vN99<v=b#Y<R=GX=,<=D=ѻ:wv]#==</UQ=={~==.GChS=\3="={]o
V<W;&dP~=Z/=5v=&A={	M `䦾c=Q=U9Bﻸ3@x=c;3<⟞<dKɻ"*;]dq5=-IŅ :QUuE	4x=\/R!3<WA<r<c=<wдe<v"=M=#ʼH{=K|T<$<â0=TAއ<=}= ^<=)L=Q<2<*!N-2<ùWp@y<<
:L1:cx<=M2a<V;V<6nSip;xAȼc=!ɼOF<<LI=ja'L<$;<R	=?<<*<<ѵ:.<];|9P;4;FL"$<2Z*Jm};{:4^ri)@mb !=Y.~u(=*=ļ L =9,;OjռB14<+=M.<8,_=^<&t;8;1<g=sn.8缞E<e=#?ԅ<Uj=8svk<LC=<-⎻ٞl<@9v:(;<+<<@{=g4{];;''Z<0@= ϻ<Uy<Mp
D;,=%;@m =x2˼ < H:h\<'kw̼ݼ& =؟+мKi<:f~2H<]:
g<CռFzqQ=P:i烽<t߼OV;E$G<[)=5[<?[>5=cنu߼Ml<;r@{:9p=K;JM0K=785=nL=EI25n<nU=.ߩI<p<=};^% =?2ƾ<#;m?<Y;&<:<5],^<S0<ʶ%<<"=i
<v#<˫=N:4;=ix&Ft=b"=ǻer=3ͼv;=}#y=;s~qn=65=ɍ\X	;>]=t2;<=;Ƽ\; =R=[q>˼t<{{:&<P;4F'8=zS=ꈃ<5.E]F=rĻT<@^<TLVM=eocKAd==4</=T2c&7Ӵ(I<qT<{<;<ʙ-=6`=)<8iV<*ꅜW<[H"ν";);;v[=4#ΣxpUw<H=:1;\2ЮBg<=G<h
=};%<H<1W5=u^=);uW=*;7=Ο	1sF'@1=|y<6M.=Bwi=-ü'Ēv֮*7;eK;~`[?K8=X=qo<`c9B~d;25F&4n<o"Y9@.P:8?3=[:8:޻"Ek4QN==]<q( ޼==Ð=N=x*o=qlx震_N=8?=.ox=4&;''<<-Լ3s=Sb<lpbɷ=<v-ߕ;bg޴<t(=fd<==[<Y8hf4N<1ټ<ԓ<%Y=g<ҲT="I<Vx<S`8=u<Tm;x<\Z;~R=O:A<<)(U>v<W](=<7섺#1U_q{0Gmv9w=4
0C">;e;$<\<M=!<PK:331;Ⱦi7=Y7!ϡ<<J{y;A7}:­өI;7=w=A㼑1=M<;A<!킽<φ=<\<e=<~Gߍ=M=*AB=SО^\Œ;y<$<<=Y־<<)$`<f?|<fRQC=s?R+ۏ<7<YQ<E ;H<=ѽg伇@ݼ</pӉx9k <P};;m=RD=-<"RSV= rĽ[<*=
!ѻȀ<k+#=s=M;t1=,B<Fv<*<=μ0M7<#Wc<]+G;`<=Y<\"
zd
=
-:^o	Rһosr=U=2a\mXT=[Z;#Xd<燼44D	\4<m<ofE=><G=AXY<:6~g]-i<wFϨ.9En
<L2<~L<@&%g=Ǥ;5<|<D㼿X-;<VY<`$x<W<(<V	k[8=`=zFZ<~'Kg<]@<\pT1<98XLZ<g|<SydѪ{K=*gx ͼ,!=˿);&<<gɼؼʻz9;;\=DK<=<N=<<]4&=<|05=)<F=h=5At<=>r-̝==49M=)>;,u*bo=Y<_=<[=f6%\C(e;<V6 2=Vn=V=͌ =
u,Jn=:==D%R"P܍<n;L=h:"1<^c~<k =lf<=8)=O/;8:%n$*߼{N=Ђ< ,*<&n9
Yi<<^[< =ȠOb)<%=b܆O=	T<H~*<8l"lt=P0<<:5;f'Ŧy%; 8o<u=DcRV[<%s;=SsmO<ʧ hԺ<:<cn=`=<,y<$<v<kZ<!< ;<9<3=1?A7<;i==;R=?=/Jk46<ng0<T<Q[<,; ;鉩;
$<Xмl,j،a켯'r<"RwO;$cj=<μT_]-[=?=*,Ť=U;
=<iG5=l<(;~b"=o礼H<_P»ٻ<ՍXڼJ^:=U~=߷<I<=߷<=ܚ<Z<<=tg,?.<e^CG=LR<DV_'b}<Mz<s8;dd=`e5E~<j<@+=o= rK<q0'=UD(չ鐼({=D;{	:=b'ܻ5KA<=.eP?;l̔e
x~~L/js꺼jР<I<44@ͼڻg9chqx=H(h="f'<܄4p&-$=K<G=̒Ui\bK;5=ӶdF<q<t=0e(=2ğIPOX=;Ov"=̺'tC>=<#=cl&4<o=nD"=ۢ;́8}<d!pcmPdE;%+=u,gۉ<Fz"<\;g =͓,=S|w=<#a(r==#;r<kj2=N<$=+9e<-<ү=;ڟ8#[8N= =w;'B<{I<-<!޽h<VI;=>⻶Dl<ƫ=:X<9=2=寻!滼33<= ٹa=:MXԪ<"<F=<==f'D=CI<HBe<Pg='a=>	4;4*<y=͈<Β4o<
Mg<{J#G=ͼgºq=X<3=;_Xj<o<f+P;e<Fʼ6u֟<k'k=D!;M`;U<HN=ۺ%
=O <gt6=DUQnO :=~=#;< ZZ9\<#C1̼uy<aE;.IG@ƼIk<LKӘB
d<fXrܼ^l=P<=oѼ̼rÌ==c?-;5=;2YX&{\*м<R:<~?%=Pּͼ|<rOj9wyTL<o`<ς;<<E<WZM<7cw<0כdd(=4=E\<L=<=/Ǽp7'<y=&;=H<W<c<<;F 97e<WtO2=q
=_.=z`_,u[9߻"Φ=3}En=d<_Q=6!#Mcuos|=k`xZv=̹;P:zϼQػ4<};?B#D; d<yw<V=e?=I;R<f</7l.<l^=T<~ۺT"=q<MiX%=ݩ<q}m[<림 D:xĺoN=E<ψ<:ȨBp.V`׼OHVӧ<<(b=f\j=E;rŦBfK<O~ϱW
z=L-?Ƽn)Ο7PoM<3O <E<Bu<KM:	<ؼ^Y<
=M¼zAμ` 㼴>:<u1==u$U<ja<us<uѾ<[oV<;q<=$\<![]f<<QྴDaA=@==-_	E@=pp< k]< =\!<v:*[b;=>ҏ<m<Z<='`=;:<iG(;А<;Yx
ҡR鼞$=!y>ƺ^<=m='e;~л8m=i*<.Zi **:+IP=b6="4<Ϗ	;-ߧ<gO(<[6D/; ;)
T>=_;ŧZ-̬@By.=Z<zʻl<-&</څeC=!_+}<r;p<o=ETռW!<	p+f=D<ݼp{"<-@=\!;S1<OP6=E=*<ky<'<<蒏:8?t<<6?<J=<[<a> 8<<<?=Ǘ>'=:=;<:<lq=I»IqDfIXZj=~=0pl< ӣ=\:#hlR:)==|@<`V==ۯ:f\:z
h"<=FTؼ>KR:<fNr[=V<!=^.c<@(=J=pgA(T;WG_==zŽ:<@1L;񼜙m`<SA,;7z<W.=Ƹ=5=]wڼo	<NՈ<~T<ǼG3.GbB=I_=WZ:Me򷻣=C\<K<AC<GD4=WjJ=T?A;2M<PL<<!=CLE<S[h=4;W^=<2żY< <;^a<p4ti<=<%1"<ƊH<y<\fߺe"~&-$oQ_ϼ?K<i;AKr߼7F'1Ҽn^='uOgc8={JԼ갼7n=!u53;X<SdF<<HyV< =HC<ޘ;FG=+<<ϼ<QS<ҞN;ע<Me1c<3;؃=Lz<-<t=:<)A<E=s;N>=<밼s==5%u1=BrRQO4z<}aA=P	<X<GO; 	<먔<ӕ=(1=Hͻ(K<噼s.p2b%<=*(¼	6I=B۫[M;oק<њdX+<<c;P
iWȽb;\;iy;Q=*;O;K;H<3-(=;ȝ=Q<ͽ<uI;5'j=z<zYz޼B`=zs8<ۊ!6) ujk?w=d<= =͖I<4V;]<`k-#P<<<EZ0a<輽%.ųʱ׼n`XU<#n>7/<=h$=O0<8`=2=d>;`9_;L`=Lq<G=;*=Ǳ=E=h޼6#=rp=Hi=/>=H=9MpG<Y<4<A^׻}<ὥsڍ;E¸o=R=|ͼJ=n=ڼ%z2Ѽ<9/<2;Hn=J"RRn[)ڼ *Pg<>;KVJ=?/=pNSuj枼<c`<_e=5!qL/"û7!=cdѼ?Kҹ}=1Ae=jX{ټ{<=na =kf= dD<-.<Z=Xv<g=' ç=6;n=;{"o<2;A׻It<b`]<;y=]5x麷Q<X <vR!Y輭ބ=}4ɼ.j=lU
;;װz<)蠽<`88=[4Y=&=<v_<|<s9Z={.;(s/L=Z<Rb<<9?XKE8u='l<{UûsTOdaRV;@5p2v;H8<nUC<I׼jѺ"U=Zi;=$G1<h۽ :μDz7=N=a=5S< Q@3qf=r߻Q&1F=<6dê\:+<ה;*< <VD=u`Ղ貲̏y8;<U%<OS<  k;r
=j77ߨ<k=0(O<S,=y<ПG^ =<ύfşe<r'=lPX;bߖ<:mib<Iā
1k=L̡v<)I=Q;7;uBkӵ;ͮ{Xf;Xv.'t*=jA;:tƼ\`<<qJ|=%&=F2e=Egr<TǛ=K N=),^j?9p=2s<@uS&<R~=7:<byi='Xp|={)U-<P"[<`.=e=ԻwJ==|t;;;r;ݻc1 D6q=KyU=cs"J<7|=\lnoxxK<8.}<Kk}m=;R<g7=W
<; /kʔmo)=66GP:=lVɼYC=QӼYʼ <zzLp<R#W=<	u<j<0="Oc:4=<%<i+ő)tG=;{ҺZL:y=0<Q<=6%2ISW3<_hq=k<A<	[Mv*=!<x qmx<Bo<YP'A?~tP<8<d=R<]=gh=&< ;r=C<'8LhB<J<y[/=J*))<+I<v	h<]=(½3=:;"ҥۼzX8m=<(;D<+=yY[Et;&<ɻz~=/"Y;.`u =3A	ކt<Kc;|˒L=bQ;l*E<nH=:@<G:<:+=`;PԠ<;Spм(7=)9>ռ'=v<w=<h<F֔ڳw=s<h/|=a73*B4e<{1;н8<h/;I=`v<js=pr<4<9m<N<Y=O<oC[;6,<:D'_<޶#=3n	 Wj<P7<]<3k;Kz%w=d<۳:=Ui:*:@3<$8;g<=<8ɽ?=V`füX<O<X.ּܸb&Jɼ0; =<f0tH=w{a;<#=Y8ė:);7Tڱ=u[=4~6<zz<p0SY<V;^<4rљHLD=@k<<DN3k=L#e;[V=,/<xo=<h;w=z<6=F(2@E=+=<Ikᦽxt=1`<7K=i;<Cpɼs{zz8Z33<ý6<DЯ80ӻ
y
"_ =8!%;Ƽh.=6euy<y3;t=k?8PͼҒ<.=E?Iټ E=v;Y<*;:I {1|98a<F=vĂ=<=v(=<<YibJ=Tn;OZ;[U|;<>;ϋ<hבֿ-=z+d:A==yc˛=8
_;J*=zI=?Yoq=»$OAG;
EX; <㛏O[<ן$l=𤻽K<s<5_/t$W<
[<KQJ
<pq5<K
<3}=g"<U4<Mܺ;]v_<4O\<T<M<,Ca;чqj2;8yD%==*:QE=	1T=Hz<jUr:y/<B<:]=ge=7,>=wZ<ZN$̼ȼ;JǼ==X~>Xp=;9j7tB:m<d(;n@<G==)=M:;[5=<.<e$=;oIߊ<=ofF=ڳ'</hNʋr=ik<W<,={<a;Ѹ_<tt=
(弒+W ҈<rȿՅ=#>=.c;&vx:;wL<м~ȼoa<eo̺AMT+f|'<"W<@=eS# ;\P;>=_;<=Oka˻<;־<M$?|;鹦<|=E=ĽZP=	x=]f5;6<.=޼=З;껭Ők&=D<.=:.|'=Y=L<.>=+9[<t;a<ei<L*4F4:w=={'Z$߼;y,JR<vii^o=e;:+<t,=ݼ8Rk^x=S!Z=nҘ<qv<z/;?5Ple=+Rb;A4OVS޻ނi!|м;F{'<I;"z{\M=;u;=9-*؆;y-=n==(H=i Sfi:^5F=u=#WUPZ=Z_Ii"<=Eü}J<	ir :I;DJ={/=Cg:S<e;W<d>2rBA<b<q=Ｓ=٢	<f$=ѹ<k;?[j;	<c+W<=)1k;s"<]c=_&1!w= M|;ҼM>;t1<0<;<, =U7<NgJ<@ռd.;Rn;dkM+<p<1:=慠lgB=JμU"<'w"=%_Rd:E<,E:|ȻF<Q<;\(=ǻ4=<GJ=1;"< yA=Zv<Դ<&R'];[=LB*:<:=rR<,4"zMtW;ļ֑<c=z<L=/<z;.<{ֻ>=s0<ڼmX<Gu  R~ƫ<:<+< lE<ڼl))㼞0gߗ<6<z.==^W=(0HL=v ==S"#˼wܼRD<G)2;Ey;;=B	戙=Hm<W%"/;=ֿ4=o}be-=S<^\+0=.=
i"讃<=I<{C=r=b<2;JE ;w<4a!]<.Iv<]B@]</J,E-!l=o=#gJ(=X< hH= %HMvPG`=Gÿ<CF=GFv=	_t;>^$=FHǻ7<?i5)e=ʇ<l&Zo;5O=s< nTFLIZ<&~3<>M݃$<<j)<:`$=ȅpLmAĬ=ć;,Dy	DL=<vu% \=qsq~Zd=bI;<A<<e;p<<=<{;=7=Y <>󜝻@$==仛<<g$^=ϼ&`:A>=hd"<+=Gh#43SelUZ={<ϓƻ4;Mb9=KOV%=F<Kvd=O>=TѼ뇼}R;=TG:><:'Q; *=<ݼLy<y%׍|=N=";3< ==!"\T=wL=(ԼLҼ	<
_=䗽$	Roޘ;ɾ`2iMO=W 1=z<:= 9	<S((=B%=8x;ḽ	-IAK=0#=w<r9v -<`(<9u=**.G$=༯&3pC<3=	=k뼆&=?ι8<;署<<<=D!=;d<dp0On<6Q=ֵ<hy<vˆ:7=7MU%L<%=L%FR<@j)+򸚹^<֍OvA⑽pa:=rg<h6꼳v|d=O7$4YLƔtV=*J:qƍH༳:ռ<R6[-=Q<"<CO=1k<9T<;6\=Sby==<_==ؼ6;<;wub3s&;4v=<+|Lu",w;뒽^N=R=Ր+=Bؼł<<B%=މ<eB:-=C̻o}Bm=t¼}pEW@;5¼|=Գ軻p߼GM_=QXb;kA<gEGּKy;⻮EQ9Im6r0=E<ͼѨ<,R==X=4=Ӽ/4B< |:<=0=(/¼U~>K;\;U4<O&= D*9@;JJ;)4=#X;zżEf=
W<
O<:O=;<4=(:Aּ<O<3<mRƼC;	򘼦tB<<2e4y<ϭL]V<&<Puׇ>=X:,n{=B?<2i=;p҆߼> >?4R;L<r c;<n<ū<<i)<}O=rO<;_;J켆A;<1=)=а]<f%D=<ZbT<3azy,=y=n
=`Ӻ*̻Q<^}=K:ҼR_7aK@</k<6;+irx4ͷ;<M<C"G3YʼW<=5=Aϕ<;<=d!f=ּ{eU:<=Vn;H3#gF=x0==?;l	<f<
6;?Tp<-#,<$<V<ˁ<9<sJ=TQ|ڼ(=g%=C;Lz;Ǡr< <A>]<cHb/+=aAUS==-<Su=H_u/`"W;i=5=1,<c`=H 2eh蝼eG<wYE;T<<d$=#?<O=|N;@==<;m4te?<i^<@=ef=s7$ x<V֐<s<=TXa<upւKLK臼*"V#=^<gN=d<=>wܵ;ƼY;ra o~;#BZ#|=|t<x+|==]=\ϼ:=3<t[=8k<dv<S|O<s5%K)=箼7<eSjX /Y'^P=b;^;M,<]hO=<i *s8H:XO2={s<#`fqP<Q;ػ@L<0=Jߘ=jpc< PP='ɼ{;];b<0<2M<9=Č<M&S=I=<;0T}=Kwt=<(ݻc<(|%Q=`Z<V<%ʨ6ۃ<oT<<|g<m92=3 = WBZ<<ǚL=7D;˗;O<c<=(l!<R;8_fA<zU)
Ӻ=VGt<H;ɼ<zP&=.=w]_=#F<l:}<j
L˄UO䙒;.
:e<^A;R{=S=j<\	:J;:7==<-lZO<:"jv+ŹS,<{gμlڐE ;ǻ1>=?<Z<c^<
g 2*w<t<+(=Ou'-XR>a <	W<<g]=JH:K~<F<G$N,/;tX=;+,*BH6޼	J)į=Jn<<膖<%
>&B=<<+@hE(<b;Fl`<ջr,<L&hc<>A=q<q=&Pq;Z=mļټ/=4<ST<ibb< <r=<k?}a|=;-e(=<+=;<1aRwވ6s<xd< u꼹5<#>X8Z[ȟ=pa=:sJ<:T-һWػ0k=`<ō"=E<ÿ"k:ﰻK/}_	o=h<K%:=&\<;
PD<==Ide<)P+=:x;&8Z7<2J<qx<b֓<'ax	D;Y*;N|1Ktٹ[<=F'<rx(=SD=*,g3]6meM== <b,<\<=<üo%5<ǧ<;ӼvE=3~{=~<<9Mԫ<׏W\<=</a:IA8H;P닼{=&˼ۼ]B=R<]G/Fq;n<~;(wB<b,=<GZ~=+߻=~;뾈= ʘ=k<=6g<.=X=˘<4i뾼'<!<a$u;kJ2NvF݁n<7j;L=;("=	b5Qr~>;
;YOu竼ox<zzN=*A7;^&X:Cb=l<<}zXA=<͞zn<E<EK<MK=5<D=1:c<;<̎;\<GZ/񹐻a<|<29BR;NhD=k\ Dǂ= =J**	=hW.=۴dE<sؽSA=hb{Ӽ*=T$<݈+<2E=]k6ͼ=c==R=1=X=<?,+cZ/3ݽd'=_e`=_UA=	~B<޴g?<NV'Sƻ=5< J;FꓼC<;O˼"\iuxXr<:"=b!廃_M2#<=ĕ<#lN=1R<!`=l7&(4܈H#wTټ?W<VϪU=-\=:)PI?o];:O=;pn=;Tf=8n=}9Jeި=X(/Ƽ =H4<4v@='=1h׼:<=:ӽgK=<_N;$=}Nal<KؽS<sJ:?==o<]*E=G=;&<(=cdHng<Dj,ۃ;1A;]$ż&<qA;qB2V=RL<O;&%]2nS;(Z2<rޚ;ptd=;/E<
ڼ&MܼxrY=Md;S;LN߀;n*<#=<}HZ$<kS=Y=*ػk<}G"<d<ɀ'<r)=˟B=BGϼ%bcG=hjty=<V߻;Z<=|
%;t<B"<M'HټUYSO;ϼ['	lB=!_Va<,e;19`; Aؼ^><iQ֋=?'f<g7k5ە<=;[\<b=t;u;x^,=%<LCPŤЧGn=dw.&H1l掼}r4Vۻ6s<~ϼ"<</;"-8R-=C<={==ᑟ;S4;Ch<@<Ǽӡ<	н;4=O\&ݷA=K$B</F<XՑa<QŤaHQ9@ڻ~ <m<8:<B=B=<<A;B=!=c}^;G+<K<7d=k$<:a9@>sĻ;1=d)Q==.4ٔ<U6=<V2;=p~t6=܎8z4<Y<&=}[-<9I:Is.=?o;9<l̼țn=.P-=dMC <#4C`:zv=L!=i<r.<@<_*XZ; ڜ
=O<I<WH<ݟ;(;"Ny^<
1<g	ýy:1`<1;(x< E <¯D<={=uG"G'rc;<Jҡ=2*=N'<'W`u?<l{P=ܼQ3<J<`ǒC;iټr<
R=aPѾҤ;w}kN˽<DX=]t=&N<),F<P[<)-;KSٻB:=(=z04<v<(`<OM;u<fU<I3D<#7⋝{;(<=A<U;*<o\<7\%k<A<vn= ?3=ɼԼlI:<M<#jcC?<K<nc+4L~\^<;p<NͻKz=9)U<%
ĳ<Q	<[7v;Xy=诽?<Scϣ<CIE=~q<;:<74Dټ	݁Q<<G&(G8=o<[]=ɟ<	>Qi<<_(z<c;<bd;<ί=;S=։0<)	X=Jzw4K<tO_+<S<$Xd\[<<)=r݋=mrʷ9>YL`5b=8m=^5?=xc<䐼E;=s\wE<<=}y8\<m<*v=YJ<p=(J=gy#=)%<8T;<'yl"
vʢw=-=܁<^=LXAͰ<;=<T;e9=gD<"V<wC=W < 5,0==796H^<U5=;9<@;W<=о9=kk;P'<=^I@Oo<=?#A<tW<|Z5 = = =i2ڶӅE=E܀X.v<j5<
Fk[L wɽ;<;=m'=N^05;ЂU&Ȼ<v$=v0ԵWxFK<lJA\ny=L;{=Wm$6O\*=>;8x=;;s =C=Jш=<i*=듼Q͞=1A<Ր<l)=%&M2<-=n=,{x=}=k4;}=$.;XWmTrl&d'=?Qۼw<掽ڼ;PV2.=;H̼7X$;Β<Gt<sμLkmAI2<i; ,Tͨ=n
=<o:ED<z=\"<=g=-۟;`<p6<8=<K7=qFb<<(<Bx#==v;_=Lb
D༁:-`>?hKj?ּ;#<S<j\<` =6<;4qy=nb <ҼmI64=K*=Ht4`)uY=v.Ӽ1S==[er:<-pmK,=pŽ;Sռuy7y9	<&Ϗ>]<#TTR=,۴=(V<&EZ=Z-<ZPS<'\;bSl9T<(<<Q<܎C<ڂ=i_o=e:y"=m3LU	+;*<HcF:ִ
=2'3=#/{˼e'< A쳰<6=l_l;r;k.e+;%]}=#)Uxw<[೪<aC=l1A<sTl=j;.="R=k<ס	<wJ=ȼJ=%.d<a96j1;A=;<<<#<-w
j+=Cq=*x#={=@<H#T=14ڼ4;JKZf/pV<E-_^<ә<"=GM=v%ۡ,YV㼻w<mp<ER=<e<	w<D=Ź%</&<$=Rt2=97=IwZ<y⇜<
2=0cCr=r<^<hwePDQ<혲<Z<#fٝ<=_<=";:\=@<&u<=S<m(%6"ĻAh<ۼ=0y<O=d=:=#m4<(~<u/m;IrR= =;*=ʻZ7|<.;;	N;=3k%sP;li67<	ؼp<Rh"=XW=Ѽ!<=%-='=424
:|p<t=X=+	/9=8<ች	=<y	k_<T=| Q3=uai˰I^=Y<*⻡=-N=`*c,<}2N^=<rЌfLe#|<^c< 9=<5=sC=A<(mN$<SQ<<<*<c;ȺQ<< ܮ<	<yz<<tn<L	xxҕ=廯I=}<kW	=k>;=&;=m_ּH=vX;F<A$R 0ټ6<厼.t,<V=Ni 8=wa<߼8Ad$=ʸZ=\1^;Fj=_<C
F<`<'J=D=>=<iA=;x,ʁ<|[<g='=<)ٺO<
*<
S;0Ȯ<N;nW</=h(Z[<p{=J5K]]z=C<*6^~<M;:;G<k<8\h1<Tl<pU6邼RΌ=r=弣;Rb+=9yV;?
nǼr,_դ<wQ</=^[G~<8<ه:I=4;ռ߼c==+5Qϛ=F<x;&!;@<WƣvBr=-H=uD=(lq<ۧu==8AK%So<v5;HEi<Qtz<<cP==;=="M,=ѻ=;<,*%q;ɍλ]<nf弙<գ<L	"衼!f=q%g廻ǻlCmd'$=<pN~Y,i<
<xisQO;p<hw6 <ҷx$8G<<UƱ<bD=VB<㘼p$MV=b2=^=;7=XǓ<P<*<.<<+=J=~PE=b4><<=W
=]N*<q|=53==A)s=IRQ.:bv<:k=Wj;>ɐA<iݼS|=*<Q=u=7=< :w% j;y<6==&;yI< <eN=E:}А<nYAy<0<2S	6%X*<p<Ø:C;UiN<El&$=XN=!*,T
<,:"=AX]0=(lN_ؿec<\	=
=ҿ;Պƻ\Q_=,|<=Ü<2μ}I*%NμO=	 =4<όͻAf6<_t/ޟR<;R<ET)<$,ٻL<U`<M̆];oF=bV	;ln<a+<qVG<sA==Hf7H;ua,Z{=?ۻDK<3;.C<c
<ju6{2#;?l+vm:遼f=R"[<N<Kq<¨<μ<)uӼX=XM4z^=D=Պ<\fԼg/<ܼܖ;;\j=j-ʼQ^Dk|@<e<ѣ=ny=z<=U,=<5<*<$cGI;
,<Mg=' >5k=2+:=ѼxY<o<(,;2e='<i!=H;8;q~؂-(G=<a
jG'|k/%/q'N=z-)z//=ȊI<:;P =RT!;h=ap{<`w <*dfKRDހO =˻F0>Ԫ}Q=1pcx4(=r[<=P<.0UV+ʋ=\=6	=[GF$e<4캉<*̊<n<Nd=α\U=lM6h)=<XL)=ަ?<⼍;"M$= <Q% ݑ{<7&s<`<R+="Ċ<\=db%Ͷl0=B<|<<qh;=}=A={~;َ=*<9ى`=R=%ڼ'=F.<+pn=]C<
=qT9KHӼ+H==wV	<v@=4<Ip`=*1ѳ<Cϊ*<GPhgJ:f<<;= =BWaJ?e=,l~=)R<^=5<d=q'=o=B疻Mu<ĸ<4=ui[ǻkO3;\|j=:0b@5=z<?F}<pZ)MӼT<2{<b<<1ûp<=:!=!&<a=e<I<yXNޣm{<M;C'<y=9/ԻD<r=}(i鿻u8&ӁzA=
2O;:MeG= =$~<<g<|;)(dBۨPݼ <k\v	YּQZՈк"ʼd<߭<n=g;K^<Q1=բ4= }= )D{޻BûD =۽( =&+=DR<.;//fX<%f<geJW<('/aӼui<ŏxrNl8$<:pK[ã;-=U;	<O<@ւIm<A2<2=b<Ʋ?==<$h(=ڌ4=%2<Y=;<"ja<aYxZIݼrE=}*=M&=b<n\@<ûy<=e=)?]ļ#= @=n;? '2=k(Q7ּ7=^C d<[<9"Ga:4<(߼+
fN<C)uJY6eR=7\<<BaLJ<vC#=<f^=J[=3=m%<b2[=<AEg=_|<ڮ逼=?	=7=<<ZJ<`<CNM=M5K<W9:<fmZ=s,fR8T:Tý;=0;ř<87"<<+,$ xzK<瀽|qㇼ&3#Ͱb "==Wh";FY<fQeL<񕼼7Ѳ<BH,<z-<@j*=_	;<;BYRajļɬ=n=SߺJ3Cx=_<ޫ͟e<f/5<-:t==0=pU;`vgJ=<Ѽ^G8:K<N\Tq*9ü5&<Eݶ;ib$<h<s	=3)UK=d<<K =ޤ 4;gp?ּ1[%<x;`		=y<3<-;
0ļ-B*&<;V3<&< =;UQ;
M:;(;<g>=,vCwE+[}=<?Լ&Bbqs}J<cǹdƻ8<Ӏ<&he<tn<<~U;^=};|;=j<MO=|[<J <S+=<=`=[= A<I<L/nm
Tn(=< H=ٔ=:;֝<7X3<_uN=ZA=0<7[|=j;4;Oӌʳ =,=rD;δ<#p7`=9$滎60<y<Hм
Ŋ;%=	8Ȝ5a0<=b=M<Y F<	W<˅=̺A4<<k<<߽RŅ΀<|ASoCμ'!DW-<ڨ<QĨ]N/Z=Ɖ뼳<L=頙&<BvfCo0=%'=}NN;2bDBEya<:~ԏ<*#'ٻ34m1=+Ln<sד<#4<4=Az<$b<F"Zߧ<<1v=ofP)<:謼U.=0I<:K=2h
;?<5;7<;=e*>;:2K;<.9=X<"AѻQg`YV[,< K_??.$;!%d=R2(<<;nE"[)=] ;(/<k<h;V|Bm%=q=<|<*.#a1:=JZmR(H<_gg<y=Uq=;Φq=g<<va9<	h꽼9^{<2<`j3fŚ=bLl^-0;q*Uk=K~5`H=;CL=qp<ۑ<Z8<i[=d<UD[mNAỀ<}v==hC⼮M-<=}Hމ<<!@<	Ż;b^</$l<Ƚn¼w^CQ>jT<I<H)rqx1m<ü&<#μH<t<nF<;<W*(5F9=
<<O=X=-$Ԓ:=aB<*<=8;ِ5_;=pм+=- L<7n}<wO=kp/};<y=<[MX;]R~"<9+=1!zCe=C#Ҽ%v<mN<ٗ9- DCn<bo=xz4==Nռ3RV&=5=λu%D=b=⊼l=o:=I٫<=.;l
J<A<zC<;^½V"2;p&0ɾ=SZ=_;
=+A= ̺e<Dlepƍӵ<2߽$FSj<<:==wiFl;;Cl=%<?w=W:=<p1<CA=z'D=	=8<ܙ7QYB<><Y=Z=FE< <z=h==RrUB;3h@<Umd<
=\Q
=Ȉ<<aQ<]:;l,<:t.D>-==fc֝=<]@h<MK;=f!v+a6&fAOp=hZOq<Xj==< [=&E d)2<}Oi<<i% roԼ_cG&^.;<"	/;7;(y:,<(<K/==d׎Y;%S= +	nHd>(<ٗ%=\ֽrc=	WN<?<[/1:U;&ⳏ33=v0<gd2s:c
2ӓ)vWڹ<m<(kQ|
=_K7<n=T<ECV<'B.=e<Լ='ż<f<q<0<O:=ZypS1`<<C<(F<X<+<|<埼pkRXp;=g<VC=~d%<s<	*ۼ<ҼOw7T=nt=)	:{<+1A<<r<e~}<]J1hX35=$Q J\g<G Z¹8<mJN3<e	*^;h S%mf$߼DyՂR=[͆N%K=E<OT=a9h<6=$B<f[f=qQ<EV?˼M=4X	=E:=</ ;q׻m}l@==Lc,Z~=-c
o<̨#r~
ޕL=_9c-z0:<%p,;Vy<A:$P;Sػz(a=Xu,+ߚ2U+=,2<;漆t6=N=?EN44=a	h8ze+<j<aμ}H z;N<;āZ=x/<Q3Y6<O'E=<bM<=5<["+&<F:==i>=B=V=pv; =%⻂O<zGûrFn<SG@,&lK?^7_:M<$<Nu;XF=k.<)@=Dxl-=}3PCM-O<E<w=Ý!<F	FOͼ<N<)A<+9>XQ::zEp%&H-=㫰;{%:nP<%<:҇Hi^ǯ;sWQ;#J=eɢ)һf<<;;7 ===ܴzT=R!=6؏	5ϸ=<;B<K<>=E<"=u=Q=h$==><q<"<"7Y2<[G;;L<ݽѼC<&}9<C?qD=\Z=v= < Q=pc<ϼԺ<|j<І<.1=ٹ=|?*;<W<#<:or<$?=w'SF<1Mg*a!<6=G'=?o}	0;n+[<	"^=@<_%߯;:/B=«'R^ ０8ÊZBYs],M=ßA_=̔:D,=΃7q<һ<C$~rHiT=Aj=`<=J	UӼY<7t<k[<r:SZU<ԃ;dkk<_=Γ<4Bj<)l<_t<짼>=Ƹ`=W9JO=BY=IV)dM=tX=Sȼ]Wq2;ߙ;1/<[=O=9<ze+1<q#s<"SѼ
&^<&J!]@=C.L<SE|cܼh,[;׼=5<MWʼ!ނ<<Z2!=5;7<mA2\ښH;<1<';ϝn=;q<(=@;;-r	ڴH<iIsJel<=Ap<;C9s@=
Zi^O<<;<=r;zϢy;=L\<&oL[A¼-ۼ<<`< <F<m};tjP['=o=N(:Ϻ;zAZ{F?=<G O`=\B;L;g<פ(_Apͻ<_7;AZHl¼y`W=C<^::S<%QCY=b9ļ9O=;K<>'=x=Nn<< irz<<Ì޼Gv<l`<*G=y(d='WDYK<i[=H/[<m<zDFk<Qn=;q>=3
7<&<ͻhP.=5:O8T|=%e<w,a򂼽u;Wx/;vY]<r=k .:|=򯓼w<zq<8_GY<0*̻Fa]<ws<÷u<	<g9r0m3>	0=)1B4/=o*@<=hmLü
{H<RZQ <ZSS+/=x<`<P2=:"W%7Z.;o<?=&
Ѽvx#A<B=K=<NR<P=w;9k3ѽՍ{="=9;/e=JTvq	u;=8e< <IVPY{Ӄɖ;)(=G/Ev<Tq=DD=`<9Z<=</;	q<H;rUK<ͼɈe=P솮;%<,/˽D_=ɨ<S=<@9d<X=Ȓ=u	=<7<Iꋼ)αt݅0<N;cʼ/{&An`ޚk]&N<=׼ݼtn=<=w-)zV<=Oۼ==aAчp<?:gA=g+=	'=0<	#KzuY<yl(<ee	=5$PT<%'<PR	mջu<vw<ž#=w=A<<%X<'X;k-KG<;a݉<<f]<Q<r<EA<^<=@=};;^<:Md2_*<C'==-˸;d<kk<Ġѻ	⪁rM==14)S =w6=f<&`N<Kȼv/;e>=#;ՠr<+-
פ%nѦ;mPNN<=<PL<= 1	=wɂ=;rȼ5ݣa*;<K@< <^=,<Y=_==M˼G̼h5d=;9=:O<5 A<`;<P+:<5ɞ<ۍ=EA=0׼zYa/o<=oL礡˼:j||c<3ϼN;<~;<y<"n<HUP~\8]ټci<d_$M<;|=w<G(L;h=B_ /<(JT<ɼL<r>Y=WT<Q=v90<b5<<$M0!̻I[b=^="-"_=;JqԻl<G;I@<s=<;<痽=<R{}3̻/\}<O5#Ytսf<lRWjh;N}|I=nYTa=<"IL='pŎ:fy,2,ד	;&Jsŋ<<|=?	S!NY=X0<Ɖ<:ף;GpE=H<2/L6^+  =?/|<d='`;?᤼1ݼ 2I؁eQWu<ZHU=At?ڼL̼sP=<H
;w<<!ѼH=ņӻcd7<:<V+9$<!=Q<B;5Z< =<TPE=/==	C=8Һ=b<9n<t+uCKl.<1<,{B=j_= ?_( =/p;7@;? =e^= =Ra=3eTػؤS="=#-9ZͼA@<%6=:*ؼ离pn&-=8<;1\y.,VӼI=^=(<}  /;<e=d䷼!}<I;=OX<\?=5V?tTHoýZ~<WWG=u<f<.M<J㮻q<h»Q(BP:<܎o=&C<ѝ]<G</Yj.]"< <e9%iH%=8q=#<(=a<l<b*<;AB<CN-_<'i<}ㆹ<=.==1qj(Fo=2<=a<4 <𴢻<=V<Q1ɾm c$@;{?΄Mfݼ.b=-<09?P){=
q<N>:9|	^qA<90t<<E<<P9,= <h<CKj<;氼[-=wc"&<=I=s̼Ҿ<$;Ƭ<H#G=;9=q-D<=.}0P=)*r 52综br[Ds=J<3\=I 	<T<͏<<PKL=0~dJ
;>-I<9)aΪ<PΤAQ=Zx<W%<y&<Cnܺt);0<~=4cqe<nD"cWp5N)&|`!D mwXbxu/ͼ<.<E={+%_^.<<HJ!kf=R	k;'=V=D=1A=ż|_<*;<	P<fK<[&l=<;$Ke7!d ,><Xa:KZ=
<JY<ʼo<xƣ7 =l_8V<8< t&=h*=禽<Zh==bWC'<Y)= -UzW0zY.༼s|=;v<z@x2=='U`Xa<%\#o>=	M=g c<r`v4<T9^=k7pb =<.Y];P=wQz=W=MTX<@5=y=kH,BJvW;r^=6.<:W~;J<;s1ڽT;ϴ<4==;< ƼM;h<BVL=-=;`Ѽ$=st?f?=m=)8&!<\C<M<`DL<*
=?&`ӻJP<%֞nPڃ$<1<VAN.,Gᠽ=ʼ<=C<F[l<=h3Җ=<X:Լ2;5[j<Pѵ=*=)?=5H=
<
a1RIP<INQp<ܼΟ&t</ɾ6<pJ"PR;I􎼘T1n̾0\x!=+&'=V=G?@y(Լs;Ӡ=n٬=}s=cgb =Ȑ+͊<~p<1=^ZpEߔ}<<;<dj<4H<{fW&=A=YH<n==<dK$a)*;hE?<Z=^4l<<2_<<ux@<j؝<7,=-u<I<E=<X)==VՋM]=m4,=<a?=R=^BkL:g	D=<<-6ּ숲<!0< )G=#Q <YL'uvog<<]̼b9=eU<zH4T</<B%=pKl<'ϕ<]aZv
?6=<gX< e=Z?i<.R<@ǿ? :i.=:- x=_{<? f=VM=,I_S:';;<ѿ{=8}<;?лV<`Wλx׼Ҥ[<~UԵ<_W(3;Ӽ4ż^=w@gz1=Ğ5R)iͮsgW='J=8W=QS_<Q4UAr!Ɯ;IM|Ȼ=y<g<)<yn4R</a;B@5 KۻI<5P=Vi<D;A=q< Ô=~=+=ӂ<-]1
|	,=j"=4?%.FH&<TD
Tto<2ۈҪ<f:~л䘼;J<;Bܥ<<<<O=j쿺h/=*< ŢJ=z<\2$;;W%]`6siJ<6vƼFZt<E:="fpzg=	ҧ-=O!<*^{n- =]]q;9ȴE=rq<<ЕH/oˬP=2i!<:=|p&<>=Lc;j<7~lh5<f&/=4=t]<䵼3c<Nk^SE=پ!ES<.=MO=&'G=Kg=U=Vw==<=<`=j<B3_=wc=̞<_,+=lZ˼v]
<r=m<FR<*e
hў<͊!L=3l6<N:=٠<֝;<qs+Nj=B=o}X6+<?ƕ'=o~:g
Y=s<e6<ȼ*<^'AJ@=:A<=R<އ;x;m,<ͬ=D~=o=Jj;7w=oZQ<煼,%üZm;;*=U:<۔w=hl(=*$0<ݝF!<1~sQ;ڪ޼w<G=sIx>=Wn=L"F=6޼Dj<O-̙>=|bU99P<LȻihh`=Z<4<xo=FfE;n .7_^L=R_=)k<Ѕb;[ӱ<38H<;۞ွ3E<)j<LEӼ:oƭ<L&ج;īY</B8?<7<puG,<;ѯ[c=۲<Qe;1op|=t<p<b/A;YλXK=N;=i<7BD<ut;=i<+ 	<ˍ!=;b~B<q6=<ϝL< {=şFO=F<Z<a:q<=Uѹ<;%o Q;Q{5a;(;f<>w=ɚ< <El==S=glmYr[<{z<Lؖ<n N, <G<x,=Mu߉93ü*c={8;`ۆ<$=c?cZz꼗ReSxGG<[C|%f'<'l˅'	lټ(g:Sϼ<8Q#<I=:Ȳ=Jo8<Aר;Ѽ<nw<kXQ<V5X	;
=yT;<>Np='G=Ƽq>i<Ke3E<]C<T9<9Qn<K=֕/=Լ^̽&&<~:
<:̢<=<Լ;wO̻v4=8_6<gP=^HڻU}5=<g?}\=ӌ9=́6
=<H;<´=\۹R<}%j;ZWM>=(<<=ϲ<<o=3;Om)<Jc7ü<}<pη:ټz=.CJ;+s=hStyz} &*<fynr;.lh<N<?׼o8ZI=b*xL;'Ѽ@F=dȫ;ɶr<=>=P,}ؼ;]F<<<g򟼷n<pW=a׼xm _B=R<.H=%=:A鯼c=ZA;͸=Q<z<e@<,a=;N\<2\CkX=?z|-G<%ycS8}=@f{O=zZw8<gye<<'<`]=<=5<ya:<M:=g<g*_L<Q<R;ؼp.<p<1qz&C&=CVxټ ?	<c;,HA<\mZ=Me(䝖</=dA<_QOqg8
<vFqo=nN34=;wdmk =:U4=SK=<\<T{2m<B<ds<2p\<2<byNW<J0;\eF޼̼1ż*|,<ϼ8<TK=ݭ<E,=A}=ZB &<9<Gn
=^@=t=D»Y<=漳j<N+=L<(m)Q'=;s' 5Qș=S%=<<<^:=˖=PO(Fn4/;IT<Y"=)q=g<мLZ: /=p\t!.8=EF(e<<=3<V1\ü<EużzkuvĤ=S<I.A=ywtFS;|<L^=ac=_VD;=l/Sݺ9ߣ=<gܻ8<CNPh]m،Ѝ1=ԙcd<8V"}=.;X3|L=2ȼGj3߼J<^j=AGU=;:=<J뽱!:}<67VPd<#i2W%=D=tɅG=|ˀ<;<=sM!Oݻ﷭:dlY<;!=}LN;=;
;<"/ee;yB<`=< n<6!B\==N8 -4=J;<3n[G`q17<E<:[<8='T˻EB =<Pˊx$V@K<V<Ӄ.s<=<aG<}mἑ.<xP%/.ώ@ּ=VnJ3==-<ۆ	żg<f<޼i
Hy=w3n~üJ=,[;`\;7l<籼ջ^<<^p=⓼O-<\<B鲍i𜼕~}'{=<H׺ i<T:;Ub9=<'=<Z}Mk+=`@=<ػ</=On:=c;=;8=~@=B =BG=B<8(*=Oюs+D<Y=i:):O;Ty;$.=~z<=VY=$<{6<،C=;UNJą=ma=.s=|%=-U@=;'A֔?2<z,=s3GX<1>;<Zai!_@M8b<4<C̼6ڇ꼽I=8=#=V1#H
f=5=]9<4cy99;H'әAb<vN=Q=H=E^^=AG=ә;;4<o+o7(c=|(<HuAѹ~Y<:#;o=<W̼aH<b񓭻YI Z=CJ;Tr=ZzXBӻp;![T=5=R<l!@=h=M<;}=O==u}T~ON<r6?2056wx{H"y<i;~"9!S<vP$=^+ȶ)<GR⌸^,Ԍ;yP[<@(?=a?<8Zoz2A=OO;"nC<^7=fNp=I:ϳ<<S<izwѼnykR=fb<_d<^5R=<j(i!}7x)<4=<.= g&Z2-@=YP<cM_<W=M<m =< B;ec=L7=t<Qa= x
Vt<lit-gpt/tutorials/inference.mdlit-gpt/tutorials/download_vicuna.mdlit-gpt/tutorials/neurips_challenge_quickstart.mdlit-gpt/tutorials/oom.mdlit-gpt/tutorials/pretrain_openwebtext.mdlit-gpt/tutorials/download_falcon.mdlit-gpt/tutorials/prepare_dataset.mdlit-gpt/tutorials/download_tinyllama.mdlit-gpt/tutorials/download_stablelm.mdlit-gpt/tutorials/download_longchat.mdlit-gpt/tutorials/download_pythia.mdlit-gpt/tutorials/resource-tables.mdlit-gpt/tutorials/finetune_adapter.mdlit-gpt/tutorials/pretrain_redpajama.mdlit-gpt/tutorials/download_phi15.mdlit-gpt/tutorials/download_openllama.mdlit-gpt/tutorials/convert_lit_models.mdlit-gpt/tutorials/download_freewilly_2.mdlit-gpt/tutorials/download_mistral.mdlit-gpt/tutorials/quantize.mdlit-gpt/tutorials/download_redpajama_incite.mdlit-gpt/tutorials/download_code_llama.mdlit-gpt/tutorials/evaluation.mdlit-gpt/tutorials/download_llama_2.mdlit-gpt/tutorials/finetune_lora.mdlit-gpt/tutorials/finetune_full.md               3     K     t                    	     /     S     w                         4     ]                              9     [     }                             }            t  
document *string8
5	embedding *fixed_size_list:float:7688
source *string8*
embedding_functions[
  {
    "name": "sentence-transformers",
    "model": {
      "name": "all-distilroberta-v1",
      "device": "cuda",
      "normalize": true
    },
    "source_column": "document",
    "vector_column": "embedding"
  }
]            LANC